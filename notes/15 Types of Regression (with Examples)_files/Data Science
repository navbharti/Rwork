// API callback
related_results_labels({"version":"1.0","encoding":"UTF-8","feed":{"xmlns":"http://www.w3.org/2005/Atom","xmlns$openSearch":"http://a9.com/-/spec/opensearchrss/1.0/","xmlns$blogger":"http://schemas.google.com/blogger/2008","xmlns$georss":"http://www.georss.org/georss","xmlns$gd":"http://schemas.google.com/g/2005","xmlns$thr":"http://purl.org/syndication/thread/1.0","id":{"$t":"tag:blogger.com,1999:blog-7958828565254404797"},"updated":{"$t":"2024-03-04T16:54:58.574-08:00"},"category":[{"term":"SAS"},{"term":"R"},{"term":"Data Science"},{"term":"Statistics"},{"term":"Python"},{"term":"Advanced Excel"},{"term":"rstats"},{"term":"VBA"},{"term":"Excel Macro"},{"term":"SAS Function"},{"term":"SAS Statistics"},{"term":"SQL"},{"term":"PROC SQL"},{"term":"Machine Learning"},{"term":"Statistics Using Excel"},{"term":"Excel Charts"},{"term":"Analytics"},{"term":"Generative AI"},{"term":"shiny"},{"term":"ChatGPT"},{"term":"Excel"},{"term":"Excel Dashboard"},{"term":"SAS Macros"},{"term":"SPSS"},{"term":"data mining"},{"term":"Credit Risk Modeling"},{"term":"Excel Functions"},{"term":"Predictive Modeling"},{"term":"Linear Regression"},{"term":"Pandas"},{"term":"Text Mining"},{"term":"Logistic Regression"},{"term":"Mathematics Using Excel"},{"term":"SAS For Beginners"},{"term":"Infographics"},{"term":"Resumes"},{"term":"SAS Certification"},{"term":"SAS Graphs"},{"term":"SAS Visual Analytics"},{"term":"4 Simple VBA Lessons"},{"term":"AI"},{"term":"Actuarial Science"},{"term":"Excel Unique Values"},{"term":"SAS Base Certification Questions and Answers"},{"term":"Time Series"},{"term":"Web Analytics"},{"term":"Web Scraping"},{"term":"random forest"},{"term":"Cluster Analysis"},{"term":"Gemini"},{"term":"Humor"},{"term":"Powerpoint"},{"term":"Text Analytics"},{"term":"SAS Interview Questions"},{"term":"Trading"},{"term":"nlp"},{"term":"Decision Tree"},{"term":"Feature Selection"},{"term":"MS Word"},{"term":"Marketing Analytics"},{"term":"R Interview Questions"},{"term":"SAS Certified Statistical Business Analyst"},{"term":"SVM"},{"term":"blogging"},{"term":"custom format"},{"term":"javascript"},{"term":"knn"},{"term":"leaflet"},{"term":"postal code"},{"term":"regex"},{"term":"regression"},{"term":"screenshot"},{"term":"selenium"},{"term":"zip code"}],"title":{"type":"text","$t":"ListenData"},"subtitle":{"type":"html","$t":"ListenData offers free analytics and data science tutorials covering topics such as SAS, R, Python, Advanced Excel, VBA, SQL, Predictive Modeling"},"link":[{"rel":"http://schemas.google.com/g/2005#feed","type":"application/atom+xml","href":"https:\/\/www.listendata.com\/feeds\/posts\/default"},{"rel":"self","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/-\/Data+Science?alt=json-in-script\u0026max-results=10"},{"rel":"alternate","type":"text/html","href":"https:\/\/www.listendata.com\/search\/label\/Data%20Science"},{"rel":"hub","href":"http://pubsubhubbub.appspot.com/"},{"rel":"next","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/-\/Data+Science\/-\/Data+Science?alt=json-in-script\u0026start-index=11\u0026max-results=10"}],"author":[{"name":{"$t":"Deepanshu Bhalla"},"uri":{"$t":"http:\/\/www.blogger.com\/profile\/09802839558125192674"},"email":{"$t":"noreply@blogger.com"},"gd$image":{"rel":"http://schemas.google.com/g/2005#thumbnail","width":"32","height":"32","src":"\/\/blogger.googleusercontent.com\/img\/b\/R29vZ2xl\/AVvXsEiXm_iOrXFR9Ls-mjtOci4qd1m1V1TXkkWJINuMy84-Axo5pNS6CG7oKwR7hfHHI3tB1yuz8W_qo9HK2Cw5fHfe_4cL_2DCf_LyoK9LMLicZojbNYgypIP-RXNsw1GsVhk\/s100\/pic.jpg"}}],"generator":{"version":"7.00","uri":"http://www.blogger.com","$t":"Blogger"},"openSearch$totalResults":{"$t":"70"},"openSearch$startIndex":{"$t":"1"},"openSearch$itemsPerPage":{"$t":"10"},"entry":[{"id":{"$t":"tag:blogger.com,1999:blog-7958828565254404797.post-1311857885835753412"},"published":{"$t":"2024-02-22T10:37:00.000-08:00"},"updated":{"$t":"2024-02-23T11:02:09.063-08:00"},"category":[{"scheme":"http://www.blogger.com/atom/ns#","term":"Data Science"},{"scheme":"http://www.blogger.com/atom/ns#","term":"Humor"}],"title":{"type":"text","$t":"Data Science Jokes"},"content":{"type":"html","$t":"\u003Cp\u003EFollowing is a list of jokes about Data Science that would make you laugh and lighten your mood.\u003C\/p\u003E\n\u003Cimg alt=\"Data Science Jokes\" data-original-height=\"603\" data-original-width=\"1198\" src=\"https:\/\/blogger.googleusercontent.com\/img\/b\/R29vZ2xl\/AVvXsEgKSTveDBRP1vS1kiEukPaTz6XXLsoEn9wgs08PGf3sl6yUcUIY9ue8r1nTW7oIirTdYmkquaY7shntiRT_5W2mTClK51Trhu_iq89DrrojPqUAeOz_mCtLSeRGee7ZOpj9KBkQdYjMfvFiB3Bi2LIMxf0I7NetvVXfG3oKmcWSwV4VZr5dsemqfZ27zF-v\/s1600\/data_science_jokes%20%281%29.png\"\u003E\n\u003Ca href=\"https:\/\/www.listendata.com\/2024\/02\/data-science-jokes.html#more\"\u003ETo read this article in full, please click here\u003C\/a\u003E\u003Cdiv class=\"blogger-post-footer\"\u003EThis post appeared first on \u003Ca href='https:\/\/www.listendata.com\/'\u003EListenData\u003C\/a\u003E\u003C\/div\u003E"},"link":[{"rel":"replies","type":"application/atom+xml","href":"https:\/\/www.listendata.com\/feeds\/1311857885835753412\/comments\/default","title":"Post Comments"},{"rel":"replies","type":"text/html","href":"https:\/\/www.listendata.com\/2024\/02\/data-science-jokes.html#comment-form","title":"0 Comments"},{"rel":"edit","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/1311857885835753412"},{"rel":"self","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/1311857885835753412"},{"rel":"alternate","type":"text/html","href":"https:\/\/www.listendata.com\/2024\/02\/data-science-jokes.html","title":"Data Science Jokes"}],"author":[{"name":{"$t":"Deepanshu Bhalla"},"uri":{"$t":"http:\/\/www.blogger.com\/profile\/09802839558125192674"},"email":{"$t":"noreply@blogger.com"},"gd$image":{"rel":"http://schemas.google.com/g/2005#thumbnail","width":"32","height":"32","src":"\/\/blogger.googleusercontent.com\/img\/b\/R29vZ2xl\/AVvXsEiXm_iOrXFR9Ls-mjtOci4qd1m1V1TXkkWJINuMy84-Axo5pNS6CG7oKwR7hfHHI3tB1yuz8W_qo9HK2Cw5fHfe_4cL_2DCf_LyoK9LMLicZojbNYgypIP-RXNsw1GsVhk\/s100\/pic.jpg"}}],"media$thumbnail":{"xmlns$media":"http://search.yahoo.com/mrss/","url":"https:\/\/blogger.googleusercontent.com\/img\/b\/R29vZ2xl\/AVvXsEgKSTveDBRP1vS1kiEukPaTz6XXLsoEn9wgs08PGf3sl6yUcUIY9ue8r1nTW7oIirTdYmkquaY7shntiRT_5W2mTClK51Trhu_iq89DrrojPqUAeOz_mCtLSeRGee7ZOpj9KBkQdYjMfvFiB3Bi2LIMxf0I7NetvVXfG3oKmcWSwV4VZr5dsemqfZ27zF-v\/s72-c\/data_science_jokes%20%281%29.png","height":"72","width":"72"},"thr$total":{"$t":"0"}},{"id":{"$t":"tag:blogger.com,1999:blog-7958828565254404797.post-2281680139010269139"},"published":{"$t":"2023-08-05T17:49:00.001-07:00"},"updated":{"$t":"2023-08-05T17:49:34.917-07:00"},"category":[{"scheme":"http://www.blogger.com/atom/ns#","term":"Data Science"},{"scheme":"http://www.blogger.com/atom/ns#","term":"R"}],"title":{"type":"text","$t":"How to Calculate AUC (Area Under Curve) in R"},"content":{"type":"html","$t":"\u003Cp\u003EIn this article we will cover how to calculate AUC (Area Under Curve) in R.\u003C\/p\u003E\n\n\u003Ch2\u003EWhat is Area Under Curve?\u003C\/h2\u003E\n\u003Cp\u003EThe Area Under Curve (AUC) is a metric used to evaluate the performance of a binary classification model. It measures the ability of a model to distinguish between events and non-events.\u003C\/p\u003E\n\u003Ca href=\"https:\/\/www.listendata.com\/2023\/08\/calculate-auc-in-r.html#more\"\u003ETo read this article in full, please click here\u003C\/a\u003E\u003Cdiv class=\"blogger-post-footer\"\u003EThis post appeared first on \u003Ca href='https:\/\/www.listendata.com\/'\u003EListenData\u003C\/a\u003E\u003C\/div\u003E"},"link":[{"rel":"replies","type":"application/atom+xml","href":"https:\/\/www.listendata.com\/feeds\/2281680139010269139\/comments\/default","title":"Post Comments"},{"rel":"replies","type":"text/html","href":"https:\/\/www.listendata.com\/2023\/08\/calculate-auc-in-r.html#comment-form","title":"0 Comments"},{"rel":"edit","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/2281680139010269139"},{"rel":"self","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/2281680139010269139"},{"rel":"alternate","type":"text/html","href":"https:\/\/www.listendata.com\/2023\/08\/calculate-auc-in-r.html","title":"How to Calculate AUC (Area Under Curve) in R"}],"author":[{"name":{"$t":"Deepanshu Bhalla"},"uri":{"$t":"http:\/\/www.blogger.com\/profile\/09802839558125192674"},"email":{"$t":"noreply@blogger.com"},"gd$image":{"rel":"http://schemas.google.com/g/2005#thumbnail","width":"32","height":"32","src":"\/\/blogger.googleusercontent.com\/img\/b\/R29vZ2xl\/AVvXsEiXm_iOrXFR9Ls-mjtOci4qd1m1V1TXkkWJINuMy84-Axo5pNS6CG7oKwR7hfHHI3tB1yuz8W_qo9HK2Cw5fHfe_4cL_2DCf_LyoK9LMLicZojbNYgypIP-RXNsw1GsVhk\/s100\/pic.jpg"}}],"media$thumbnail":{"xmlns$media":"http://search.yahoo.com/mrss/","url":"https:\/\/blogger.googleusercontent.com\/img\/b\/R29vZ2xl\/AVvXsEjhApq1sW6tLaFEXU4jK9g5YzvQBsudIhRUNkSBSYldIPaQ1Rwny1Z7Sj-Rt_8W2CCVKGCMDqu5GoSu4Cao1rAkfwVjgPfINcRuJTGjR-JPXbxY7NtgKQbFYUT2z6IfOAaeZadKVjdvQlkSG7JbDntxsUXWatk1tOUk0XPGBzbnKIRL6f929n8u3Pk1alQf\/s72-c\/roc.png","height":"72","width":"72"},"thr$total":{"$t":"0"}},{"id":{"$t":"tag:blogger.com,1999:blog-7958828565254404797.post-2811903644401203682"},"published":{"$t":"2020-10-11T07:45:00.021-07:00"},"updated":{"$t":"2023-07-13T21:38:47.531-07:00"},"category":[{"scheme":"http://www.blogger.com/atom/ns#","term":"Data Science"},{"scheme":"http://www.blogger.com/atom/ns#","term":"Python"}],"title":{"type":"text","$t":"Python for Data Science: Beginner's Guide"},"content":{"type":"html","$t":"\u003Cp\u003EThis tutorial would help you to learn Data Science with Python by examples. It is designed for beginners who want to get started with Data Science in Python. Python is an open source language and it is widely used as a high-level programming language for general-purpose programming. It has gained high popularity in data science world. In the PyPL Popularity of Programming language index, Python leads with a 29 percent share. In advanced analytics and predictive analytics market, it is ranked among top 3 programming languages for advanced analytics.\u003C\/p\u003E\n\u003Ctable align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Ca href=\"https:\/\/2.bp.blogspot.com\/-YZGskcb-Puo\/WpAaxUACl7I\/AAAAAAAAGvE\/F5KJhbQ2nDgnXivIIyhwPoD6hqgiyGY-wCLcBGAs\/s1600\/Data%2BScience%2BPython.PNG\" style=\"margin-left: auto; margin-right: auto;\"\u003E\u003Cimg alt=\"Data Science Python\" border=\"0\" data-original-height=\"221\" data-original-width=\"459\" src=\"https:\/\/2.bp.blogspot.com\/-YZGskcb-Puo\/WpAaxUACl7I\/AAAAAAAAGvE\/F5KJhbQ2nDgnXivIIyhwPoD6hqgiyGY-wCLcBGAs\/s1600\/Data%2BScience%2BPython.PNG\" title=\"Data Science Python\"\u003E\u003C\/a\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003Ctr\u003E\u003Ctd class=\"tr-caption\" style=\"text-align: center;\"\u003EData Science with Python Tutorial\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\n\u003Cdiv id=\"toc\"\u003E\n\u003Cspan\u003ETable of Contents\u003C\/span\u003E\u003C\/div\u003E\n\u003Chr\u003E\n\u003Cdiv id=\"contents\"\u003E\n  \n\u003Ch2 style='margin-top: 1em;'\u003EIntroduction\u003C\/h2\u003E\nPython is widely used and very popular for a variety of software engineering tasks such as website development, cloud-architecture, back-end etc. It is equally popular in data science world. In advanced analytics world, there has been several debates on R vs. Python. There are some areas such as number of libraries for statistical analysis, where R wins over Python but Python is catching up very fast.With popularity of big data and data science, Python has become first programming language of data scientists.\n\u003Cp\u003EThere are several reasons to learn Python. Some of them are as follows -\u003C\/p\u003E\n\u003Col\u003E\n\u003Cli\u003EPython runs well in automating various steps of a predictive model.\u003C\/li\u003E\n\u003Cli\u003EPython has awesome robust libraries for machine learning, natural language processing, deep learning, big data and artificial Intelligence.\u003C\/li\u003E\n\u003Cli\u003EPython wins over R when it comes to deploying machine learning models in production.\u003C\/li\u003E\n\u003Cli\u003EIt can be easily integrated with big data frameworks such as Spark and Hadoop.\u003C\/li\u003E\n\u003Cli\u003EPython has a great online community support.\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003Cdiv class=\"db2\"\u003EDo you know these sites are developed in Python?\u003C\/div\u003E\n\u003Col style=\"text-align: left;\"\u003E\n\u003Cli\u003EYouTube\u003C\/li\u003E\n\u003Cli\u003EInstagram\u003C\/li\u003E\n\u003Cli\u003EReddit\u003C\/li\u003E\n\u003Cli\u003EDropbox\u003C\/li\u003E\n\u003Cli\u003EDisqus\u003C\/li\u003E\n\u003C\/ol\u003E\n\n\n\u003Ch2 style='margin-top: 1em;'\u003EPython 2 vs. 3\u003C\/h2\u003E\nGoogle yields thousands of articles on this topic. Some bloggers opposed and some in favor of 2.7. If you filter your search criteria and look for only recent articles, you would find Python 2 is no longer supported by the Python Software Foundation. Hence it does not make any sense to learn 2.7 if you start learning it today. Python 3 supports all the packages. Python 3 is cleaner and faster. It is a language for the future. It fixed major issues with versions of Python 2 series. Python 3 was first released in year 2008. It has been 12 years releasing robust versions of Python 3 series. You should go for latest version of Python 3.\n  \n\u003Ch2 style='margin-top: 1em;'\u003EHow to install Python?\u003C\/h2\u003E\nThere are two ways to download and install Python\u003Cbr\u003E\n\u003Col style=\"text-align: left;\"\u003E\n\u003Cli\u003E\u003Cb\u003E\u003Ca href=\"https:\/\/www.anaconda.com\/products\/individual\" rel=\"nofollow\" target=\"_blank\"\u003EDownload Anaconda\u003C\/a\u003E. \u003C\/b\u003EIt comes with Python software along with preinstalled popular libraries.\u003C\/li\u003E\n\u003Cli\u003EDownload \u003Cb\u003E\u003Ca href=\"https:\/\/www.python.org\/downloads\/\" rel=\"nofollow\" target=\"_blank\"\u003EPython\u003C\/a\u003E\u003C\/b\u003Efrom its official website. You have to manually install libraries.\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003Cblockquote\u003E\u003Cb\u003ERecommended :\u003C\/b\u003E Go for first option and download anaconda. It saves a lot of time in learning and coding Python\n\u003C\/blockquote\u003E\n\u003Cdiv class=\"db\"\u003ECoding Environments\u003C\/div\u003E\nAnaconda comes with two popular IDE :\n\u003Col style=\"text-align: left;\"\u003E\n\u003Cli\u003EJupyter (Ipython) Notebook\u003C\/li\u003E\n\u003Cli\u003ESpyder\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003Cb\u003ESpyder\u003C\/b\u003E. It is like RStudio for Python. It gives an environment wherein writing python code is user-friendly. If you are a SAS User, you can think of it as SAS Enterprise Guide \/ SAS Studio. It comes with a syntax editor where you can write programs. It has a console to check each and every line of code. Under the \u0026#39;Variable explorer\u0026#39;, you can access your created data files and function. \u003Cb\u003EI highly recommend Spyder!\u003C\/b\u003E\u003Ctable align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Ca href=\"https:\/\/3.bp.blogspot.com\/--7uGoD3tH2s\/WSHT5tEN9CI\/AAAAAAAAGQs\/QAGri0KVnvcTVrKdbbFJ_XkNkY5kjfdlACLcB\/s1600\/Spyder.png\" style=\"margin-left: auto; margin-right: auto;\"\u003E\u003Cimg border=\"0\" src=\"https:\/\/3.bp.blogspot.com\/--7uGoD3tH2s\/WSHT5tEN9CI\/AAAAAAAAGQs\/QAGri0KVnvcTVrKdbbFJ_XkNkY5kjfdlACLcB\/s1600\/Spyder.png\"\u003E\u003C\/a\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003Ctr\u003E\u003Ctd class=\"tr-caption\" style=\"text-align: center;\"\u003ESpyder - Python Coding Environment\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Cb\u003EJupyter (Ipython) Notebook\u003C\/b\u003E Jupyter is equivalent to markdown in R. It is useful when you need to present your work to others or when you need to create step by step project report as it can combine code, output, words, and graphics.\n\u003Cbr\u003E\n\u003C\/div\u003E\u003Ca href=\"https:\/\/www.listendata.com\/2020\/10\/learn-python-for-data-science.html#more\"\u003ETo read this article in full, please click here\u003C\/a\u003E\u003Cdiv class=\"blogger-post-footer\"\u003EThis post appeared first on \u003Ca href='https:\/\/www.listendata.com\/'\u003EListenData\u003C\/a\u003E\u003C\/div\u003E"},"link":[{"rel":"edit","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/2811903644401203682"},{"rel":"self","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/2811903644401203682"},{"rel":"alternate","type":"text/html","href":"https:\/\/www.listendata.com\/2020\/10\/learn-python-for-data-science.html","title":"Python for Data Science: Beginner's Guide"}],"author":[{"name":{"$t":"Deepanshu Bhalla"},"uri":{"$t":"http:\/\/www.blogger.com\/profile\/09802839558125192674"},"email":{"$t":"noreply@blogger.com"},"gd$image":{"rel":"http://schemas.google.com/g/2005#thumbnail","width":"32","height":"32","src":"\/\/blogger.googleusercontent.com\/img\/b\/R29vZ2xl\/AVvXsEiXm_iOrXFR9Ls-mjtOci4qd1m1V1TXkkWJINuMy84-Axo5pNS6CG7oKwR7hfHHI3tB1yuz8W_qo9HK2Cw5fHfe_4cL_2DCf_LyoK9LMLicZojbNYgypIP-RXNsw1GsVhk\/s100\/pic.jpg"}}],"media$thumbnail":{"xmlns$media":"http://search.yahoo.com/mrss/","url":"https:\/\/2.bp.blogspot.com\/-YZGskcb-Puo\/WpAaxUACl7I\/AAAAAAAAGvE\/F5KJhbQ2nDgnXivIIyhwPoD6hqgiyGY-wCLcBGAs\/s72-c\/Data%2BScience%2BPython.PNG","height":"72","width":"72"}},{"id":{"$t":"tag:blogger.com,1999:blog-7958828565254404797.post-7928240905418656697"},"published":{"$t":"2019-09-26T00:51:00.002-07:00"},"updated":{"$t":"2020-11-15T04:41:16.077-08:00"},"category":[{"scheme":"http://www.blogger.com/atom/ns#","term":"Data Science"},{"scheme":"http://www.blogger.com/atom/ns#","term":"Marketing Analytics"}],"title":{"type":"text","$t":"A Complete Guide to Marketing Mix Modeling"},"content":{"type":"html","$t":"Marketing Mix Modeling (MMM) is one of the most popular analysis under Marketing Analytics which helps organisations in estimating the effects of spent on different advertising channels (TV, Radio, Print, Online Ads etc) as well as other factors (price, competition, weather, inflation, unemployment) on sales. In simple words, it helps companies in optimizing their marketing investments which they spent in different marketing mediums (both online and offline).\n\u003Cbr\u003E\n\u003Cdiv style=\"text-align: center;\"\u003E\u003Ca href=\"https:\/\/2.bp.blogspot.com\/-9SHnkFk8hUc\/XYxsSbjr6YI\/AAAAAAAAIMg\/bkovKQQFQaU2rfpd_dEMbLpo11Ul6Il6gCLcBGAsYHQ\/s1600\/marketing%2Bmix%2Bmodeling.PNG\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Cimg border=\"0\" src=\"https:\/\/2.bp.blogspot.com\/-9SHnkFk8hUc\/XYxsSbjr6YI\/AAAAAAAAIMg\/bkovKQQFQaU2rfpd_dEMbLpo11Ul6Il6gCLcBGAsYHQ\/s1600\/marketing%2Bmix%2Bmodeling.PNG\" data-original-width=\"528\" data-original-height=\"278\"\u003E\u003C\/a\u003E\u003C\/div\u003E\n\u003Cdiv class=\"toplist\"\u003E\n\u003Cdiv class=\"toplist_head\"\u003EUses of Marketing Mix Modeling\u003C\/div\u003E\nIt answers the following questions which management generally wants to know.\n\u003Col\u003E\n\u003Cli\u003EWhich marketing medium (TV, radio, print, online ads) returns maximum return (ROI)?\u003C\/li\u003E\n\u003Cli\u003EHow much to spend on marketing activities to increase sales by some percent (15%)?\u003C\/li\u003E\n\u003Cli\u003EPredict sales in future from investment spent on marketing activities\u003C\/li\u003E\n\u003Cli\u003EIdentifying Key drivers of sales (including marketing mediums, price, competition, weather and macro-economic factors)\u003C\/li\u003E\n\u003Cli\u003EHow to optimize marketing spend?\u003C\/li\u003E\n\u003Cli\u003EIs online marketing medium better than offline?\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003C\/div\u003E\n\u003Cp\u003EImagine you are a chief strategist and your role requires you to come up with strategies that can boost company\u0026#39;s revenue growth and profitability. With the use of insights from marketing mix model, you know sales can be increased by 50 million dollars for every 5 million dollars you spend on advertising. MMM would also help you to determine how much to spend on each advertising medium to get maximum return on investment (ROI).\u003C\/p\u003E\n\u003Ca href=\"https:\/\/www.listendata.com\/2019\/09\/marketing-mix-modeling.html#more\"\u003ETo read this article in full, please click here\u003C\/a\u003E\u003Cdiv class=\"blogger-post-footer\"\u003EThis post appeared first on \u003Ca href='https:\/\/www.listendata.com\/'\u003EListenData\u003C\/a\u003E\u003C\/div\u003E"},"link":[{"rel":"replies","type":"application/atom+xml","href":"https:\/\/www.listendata.com\/feeds\/7928240905418656697\/comments\/default","title":"Post Comments"},{"rel":"replies","type":"text/html","href":"https:\/\/www.listendata.com\/2019\/09\/marketing-mix-modeling.html#comment-form","title":"26 Comments"},{"rel":"edit","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/7928240905418656697"},{"rel":"self","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/7928240905418656697"},{"rel":"alternate","type":"text/html","href":"https:\/\/www.listendata.com\/2019\/09\/marketing-mix-modeling.html","title":"A Complete Guide to Marketing Mix Modeling"}],"author":[{"name":{"$t":"Deepanshu Bhalla"},"uri":{"$t":"http:\/\/www.blogger.com\/profile\/09802839558125192674"},"email":{"$t":"noreply@blogger.com"},"gd$image":{"rel":"http://schemas.google.com/g/2005#thumbnail","width":"32","height":"32","src":"\/\/blogger.googleusercontent.com\/img\/b\/R29vZ2xl\/AVvXsEiXm_iOrXFR9Ls-mjtOci4qd1m1V1TXkkWJINuMy84-Axo5pNS6CG7oKwR7hfHHI3tB1yuz8W_qo9HK2Cw5fHfe_4cL_2DCf_LyoK9LMLicZojbNYgypIP-RXNsw1GsVhk\/s100\/pic.jpg"}}],"media$thumbnail":{"xmlns$media":"http://search.yahoo.com/mrss/","url":"https:\/\/2.bp.blogspot.com\/-9SHnkFk8hUc\/XYxsSbjr6YI\/AAAAAAAAIMg\/bkovKQQFQaU2rfpd_dEMbLpo11Ul6Il6gCLcBGAsYHQ\/s72-c\/marketing%2Bmix%2Bmodeling.PNG","height":"72","width":"72"},"thr$total":{"$t":"26"}},{"id":{"$t":"tag:blogger.com,1999:blog-7958828565254404797.post-421859835313809710"},"published":{"$t":"2019-09-02T08:21:00.002-07:00"},"updated":{"$t":"2019-09-04T08:29:15.003-07:00"},"category":[{"scheme":"http://www.blogger.com/atom/ns#","term":"Credit Risk Modeling"},{"scheme":"http://www.blogger.com/atom/ns#","term":"Data Science"}],"title":{"type":"text","$t":"Gini, Cumulative Accuracy Profile, AUC"},"content":{"type":"html","$t":"\u003Cdiv dir=\"ltr\" style=\"text-align: left;\" trbidi=\"on\"\u003E\nIn this article, we have covered how to calculate Gini Coefficient, Cumulative Accuracy Profile (CAP) and Area under Curve (AUC) of a predictive model. The purpose of this article is to explain these concepts in simple terms so that layman can understand the mathematics behind it.\n\n\u003Cdiv class=\"db\"\u003EImportance of these methods\u003C\/div\u003E \nThese methods measure discriminatory power of a predictive model. Discriminatory power implies whether predictive model is able to distinguish between events (desired outcome) and non-events. In \u003Ca target=\"_blank\" href=\"https:\/\/www.listendata.com\/2019\/08\/credit-risk-modelling.html\"\u003Ecredit risk modeling\u003C\/a\u003E, it evaluates whether the probability of default model is able to separate good and bad customers. These two metrics Cumulative Accuracy Profile and Gini Coefficient are more common in credit risk analytics as compared to other domains.\n\u003Cbr\u003E\u003Cdiv id=\"toc\"\u003E\n\u003Cspan\u003ETable of Contents\u003C\/span\u003E\u003C\/div\u003E\n\u003Chr\u003E\n\u003Cdiv id=\"contents\"\u003E\n\u003Ch2 style=\"margin-top: 1em;\"\u003ECumulative Accuracy Profile (CAP)\u003C\/h2\u003E\nCumulative Accuracy profile (CAP) of a credit rating model shows percentage of all borrowers (debtors) on the x-axis and the percentage of defaulters (bad customers) on the y-axis. In marketing analytics, it is called \u003Ccode\u003E\u003Ca href=\"https:\/\/www.listendata.com\/2014\/08\/excel-template-gain-and-lift-charts.html\"  target='_blank'\u003EGain Chart\u003C\/a\u003E\u003C\/code\u003E. It is also called Power Curve in some other domains.\n\n\u003Cdiv class=\"separator\" style=\"clear: both; text-align: center;\"\u003E\u003Ca href=\"https:\/\/4.bp.blogspot.com\/-Z91ISbjxfYY\/XWmOIgUy8gI\/AAAAAAAAIE0\/OVN8xgWxlLcyUQKDTp6oeA5NwCz7heKVgCLcBGAs\/s1600\/accuracy%2Bratio.PNG\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Cimg alt=\"Accuracy Ratio\" border=\"0\" src=\"https:\/\/4.bp.blogspot.com\/-Z91ISbjxfYY\/XWmOIgUy8gI\/AAAAAAAAIE0\/OVN8xgWxlLcyUQKDTp6oeA5NwCz7heKVgCLcBGAs\/s1600\/accuracy%2Bratio.PNG\" data-original-width=\"662\" data-original-height=\"241\"\u003E\u003C\/a\u003E\u003C\/div\u003E\n\n\u003Cdiv class=\"db2\"\u003EInterpretation\u003C\/div\u003E\nBy using CAP, you can compare the curve of your current model to the curve of \u0026#39;ideal or perfect\u0026#39; model and can also compare it with the curve of random model. \u0026#39;Perfect model\u0026#39; refers to the ideal state in which all the bad customers (desired outcome) can be captured directly. \u0026#39;Random model\u0026#39; refers to the state in which the proportion of bad customers are distributed equally. \u0026#39;Current Model\u0026#39; refers to your probability of default model (or any other model you are working on). We always try to build the model which leans toward (closer) to the curve of perfect model. We can read current model as \u0026#39;% of bad customers covered at a given decile level\u0026#39;. For example,  89% of bad customers captured by just selecting top 30% of debtors based on model.\n\u003C\/div\u003E\u003C\/div\u003E\u003Ca href=\"https:\/\/www.listendata.com\/2019\/09\/gini-cumulative-accuracy-profile-auc.html#more\"\u003ETo read this article in full, please click here\u003C\/a\u003E\u003Cdiv class=\"blogger-post-footer\"\u003EThis post appeared first on \u003Ca href='https:\/\/www.listendata.com\/'\u003EListenData\u003C\/a\u003E\u003C\/div\u003E"},"link":[{"rel":"replies","type":"application/atom+xml","href":"https:\/\/www.listendata.com\/feeds\/421859835313809710\/comments\/default","title":"Post Comments"},{"rel":"replies","type":"text/html","href":"https:\/\/www.listendata.com\/2019\/09\/gini-cumulative-accuracy-profile-auc.html#comment-form","title":"8 Comments"},{"rel":"edit","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/421859835313809710"},{"rel":"self","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/421859835313809710"},{"rel":"alternate","type":"text/html","href":"https:\/\/www.listendata.com\/2019\/09\/gini-cumulative-accuracy-profile-auc.html","title":"Gini, Cumulative Accuracy Profile, AUC"}],"author":[{"name":{"$t":"Deepanshu Bhalla"},"uri":{"$t":"http:\/\/www.blogger.com\/profile\/09802839558125192674"},"email":{"$t":"noreply@blogger.com"},"gd$image":{"rel":"http://schemas.google.com/g/2005#thumbnail","width":"32","height":"32","src":"\/\/blogger.googleusercontent.com\/img\/b\/R29vZ2xl\/AVvXsEiXm_iOrXFR9Ls-mjtOci4qd1m1V1TXkkWJINuMy84-Axo5pNS6CG7oKwR7hfHHI3tB1yuz8W_qo9HK2Cw5fHfe_4cL_2DCf_LyoK9LMLicZojbNYgypIP-RXNsw1GsVhk\/s100\/pic.jpg"}}],"media$thumbnail":{"xmlns$media":"http://search.yahoo.com/mrss/","url":"https:\/\/4.bp.blogspot.com\/-Z91ISbjxfYY\/XWmOIgUy8gI\/AAAAAAAAIE0\/OVN8xgWxlLcyUQKDTp6oeA5NwCz7heKVgCLcBGAs\/s72-c\/accuracy%2Bratio.PNG","height":"72","width":"72"},"thr$total":{"$t":"8"}},{"id":{"$t":"tag:blogger.com,1999:blog-7958828565254404797.post-5032248021631054653"},"published":{"$t":"2019-07-29T13:20:00.000-07:00"},"updated":{"$t":"2019-07-29T14:24:46.071-07:00"},"category":[{"scheme":"http://www.blogger.com/atom/ns#","term":"Data Science"},{"scheme":"http://www.blogger.com/atom/ns#","term":"Python"}],"title":{"type":"text","$t":"Precision Recall Curve Simplified"},"content":{"type":"html","$t":"\u003Cdiv dir=\"ltr\" style=\"text-align: left;\" trbidi=\"on\"\u003E\nThis article outlines precision recall curve and how it is used in real-world data science application. It includes explanation of how it is different from ROC curve. It also highlights limitation of ROC curve and how it can be solved via area under precision-recall curve. This article also covers implementation of area under precision recall curve in Python, R and SAS.\n\u003Cdiv id=\"toc\"\u003E\n\u003Cspan\u003ETable of Contents\u003C\/span\u003E\u003C\/div\u003E\n\u003Chr\u003E\n\u003Cdiv id=\"contents\"\u003E\n\u003Ch2\u003EWhat is Precision Recall Curve?\u003C\/h2\u003E\nBefore getting into technical details, we first need to understand precision and recall terms in layman\u0026#39;s term. It is essential to understand the concepts in simple words so that you can recall it for future work when it is required. Both Precision and Recall are important metrics to check the performance of binary classification model.\n\n\u003Ch3 style=\"margin-top:1em;\"\u003EPrecision\u003C\/h3\u003E\nPrecision is also called \u003Cb\u003EPositive Predictive Value\u003C\/b\u003E. Suppose you are building a customer attrition model which has objective to identify customers who are likely to close relationship with the company. The use of this model is to prevent attrition and boost customer profitability.\nIt\u0026#39;s a binary classification problem in which dependent variable is binary in nature. It has only two values either 0 or 1. 1 refers to customers who left us. 0 refers to active customers who are still with us. In this case, \u003Cb\u003Eprecision is the proportion of customers our predictive model call as attritors actually left us (attrited).\u003C\/b\u003E\n\u003Cp\u003ELet\u0026#39;s understand it by confusion matrix\u003C\/p\u003E\n\u003Cdiv class=\"separator\" style=\"clear: both; text-align: center;\"\u003E\u003Ca href=\"https:\/\/3.bp.blogspot.com\/-IBRvfmzMgGk\/XT701l3bPUI\/AAAAAAAAH8w\/5AiCWm0pMOcOI_TdGwV6mZAopxdVKhVfACLcBGAs\/s1600\/Confusion%2BMatrix.PNG\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Cimg alt=\"confusion matrix\" border=\"0\" src=\"https:\/\/3.bp.blogspot.com\/-IBRvfmzMgGk\/XT701l3bPUI\/AAAAAAAAH8w\/5AiCWm0pMOcOI_TdGwV6mZAopxdVKhVfACLcBGAs\/s1600\/Confusion%2BMatrix.PNG\" data-original-width=\"371\" data-original-height=\"96\"\u003E\u003C\/a\u003E\u003C\/div\u003E\n\u003Cblockquote\u003EPrecision = True Positive \/ (True Positive + False Positive)\n\u003Cul\u003E\n\u003Cli\u003ETrue Positive : Number of customers who actually attrited whom we correctly predicted as attritors.\u003C\/li\u003E\n\u003Cli\u003EFalse Positive : Number of customers who actually did not attrite whom we incorrectly predicted as attritors.\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003C\/blockquote\u003E\n\u003C\/div\u003E\u003C\/div\u003E\u003Ca href=\"https:\/\/www.listendata.com\/2019\/07\/precision-recall-curve-simplified.html#more\"\u003ETo read this article in full, please click here\u003C\/a\u003E\u003Cdiv class=\"blogger-post-footer\"\u003EThis post appeared first on \u003Ca href='https:\/\/www.listendata.com\/'\u003EListenData\u003C\/a\u003E\u003C\/div\u003E"},"link":[{"rel":"replies","type":"application/atom+xml","href":"https:\/\/www.listendata.com\/feeds\/5032248021631054653\/comments\/default","title":"Post Comments"},{"rel":"replies","type":"text/html","href":"https:\/\/www.listendata.com\/2019\/07\/precision-recall-curve-simplified.html#comment-form","title":"2 Comments"},{"rel":"edit","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/5032248021631054653"},{"rel":"self","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/5032248021631054653"},{"rel":"alternate","type":"text/html","href":"https:\/\/www.listendata.com\/2019\/07\/precision-recall-curve-simplified.html","title":"Precision Recall Curve Simplified"}],"author":[{"name":{"$t":"Deepanshu Bhalla"},"uri":{"$t":"http:\/\/www.blogger.com\/profile\/09802839558125192674"},"email":{"$t":"noreply@blogger.com"},"gd$image":{"rel":"http://schemas.google.com/g/2005#thumbnail","width":"32","height":"32","src":"\/\/blogger.googleusercontent.com\/img\/b\/R29vZ2xl\/AVvXsEiXm_iOrXFR9Ls-mjtOci4qd1m1V1TXkkWJINuMy84-Axo5pNS6CG7oKwR7hfHHI3tB1yuz8W_qo9HK2Cw5fHfe_4cL_2DCf_LyoK9LMLicZojbNYgypIP-RXNsw1GsVhk\/s100\/pic.jpg"}}],"media$thumbnail":{"xmlns$media":"http://search.yahoo.com/mrss/","url":"https:\/\/3.bp.blogspot.com\/-IBRvfmzMgGk\/XT701l3bPUI\/AAAAAAAAH8w\/5AiCWm0pMOcOI_TdGwV6mZAopxdVKhVfACLcBGAs\/s72-c\/Confusion%2BMatrix.PNG","height":"72","width":"72"},"thr$total":{"$t":"2"}},{"id":{"$t":"tag:blogger.com,1999:blog-7958828565254404797.post-569869026088308085"},"published":{"$t":"2018-05-30T12:45:00.000-07:00"},"updated":{"$t":"2019-06-21T02:07:18.401-07:00"},"category":[{"scheme":"http://www.blogger.com/atom/ns#","term":"Data Science"},{"scheme":"http://www.blogger.com/atom/ns#","term":"nlp"},{"scheme":"http://www.blogger.com/atom/ns#","term":"Python"}],"title":{"type":"text","$t":"Identify Person, Place and Organisation in content using Python"},"content":{"type":"html","$t":"\u003Cdiv dir=\"ltr\" style=\"text-align: left;\" trbidi=\"on\"\u003E\nThis article outlines the concept and python implementation of \u003Cb\u003ENamed Entity Recognition\u003C\/b\u003E using StanfordNERTagger. The technical challenges such as installation issues, version conflict issues, operating system issues that are very common to this analysis are out of scope for this article.\u003Cbr \/\u003E\n\u003Cbr \/\u003E\n\u003Ctable align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Ca href=\"https:\/\/1.bp.blogspot.com\/-ZpwUNrrzGDg\/Ww8BiYr0hjI\/AAAAAAAAHO4\/q77lafZmmqEUlLntQdKCwwVnnDx3IwAPQCLcBGAs\/s1600\/nlp.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"\u003E\u003Cimg border=\"0\" data-original-height=\"217\" data-original-width=\"438\" src=\"https:\/\/1.bp.blogspot.com\/-ZpwUNrrzGDg\/Ww8BiYr0hjI\/AAAAAAAAHO4\/q77lafZmmqEUlLntQdKCwwVnnDx3IwAPQCLcBGAs\/s1600\/nlp.png\" \/\u003E\u003C\/a\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003Ctr\u003E\u003Ctd class=\"tr-caption\" style=\"text-align: center;\"\u003ENER NLP using Python\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Cbr \/\u003E\n\u003Cspan style=\"color: #990000; font-size: x-large;\"\u003E\u003Cb\u003ETable of contents:\u003C\/b\u003E\u003C\/span\u003E\u003Cbr \/\u003E\n\u003Cbr \/\u003E\n1. Named Entity Recognition defined\u003Cbr \/\u003E\n2. Business Use cases\u003Cbr \/\u003E\n3. Installation Pre-requisites\u003Cbr \/\u003E\n4. Python Code for implementation\u003Cbr \/\u003E\n5. Additional Reading: CRF model, Multiple models available in the package\u003Cbr \/\u003E\n6. Disclaimer\u003Cbr \/\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cdiv\u003E\n\u003Cspan style=\"color: #990000; font-size: x-large;\"\u003E\u003Cb\u003E1. Named Entity Recognition Defined\u003C\/b\u003E\u003C\/span\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cblockquote class=\"tr_bq\"\u003E\nThe process of detecting and classifying proper names mentioned in a text can be defined as \u003Cb\u003ENamed Entity Recognition (NER)\u003C\/b\u003E. In simple words, it locates person name, organization and location etc. in the content. This is generally the first step in most of the Information Extraction (IE) tasks of Natural Language Processing.\u003C\/blockquote\u003E\n\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Ctable align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Ca href=\"https:\/\/3.bp.blogspot.com\/-4fq9AYAJi8A\/Ww79qGdh5eI\/AAAAAAAAHOs\/nqlP4RSnriIr0XXF7x3fX9HXhvUE5Ew_ACLcBGAs\/s1600\/NER.PNG\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"\u003E\u003Cimg border=\"0\" data-original-height=\"116\" data-original-width=\"468\" src=\"https:\/\/3.bp.blogspot.com\/-4fq9AYAJi8A\/Ww79qGdh5eI\/AAAAAAAAHOs\/nqlP4RSnriIr0XXF7x3fX9HXhvUE5Ew_ACLcBGAs\/s1600\/NER.PNG\" \/\u003E\u003C\/a\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003Ctr\u003E\u003Ctd class=\"tr-caption\" style=\"text-align: center;\"\u003ENER Sample\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cspan style=\"color: #990000; font-size: x-large;\"\u003E\u003Cb\u003E2. Business Use Cases\u003C\/b\u003E\u003C\/span\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\nThere is a need for NER across multiple domains. Below are a few sample business use cases for your reference.\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Col style=\"text-align: left;\"\u003E\n\u003Cli\u003E\u003Cb\u003EInvestment research:\u003C\/b\u003E To identify the various announcements of the companies, people’s reaction towards them and its impact on the stock prices, one needs to identify people and organisation names in the text\u003C\/li\u003E\n\u003Cli\u003E\u003Cb\u003EChat-bots in multiple domains:\u003C\/b\u003E To identify places and dates for booking hotel rooms, air tickets etc.\u003C\/li\u003E\n\u003Cli\u003E\u003Cb\u003EInsurance domain:\u003C\/b\u003E Identify and mask people’s names in the feedback forms before analyzing. This is needed for being regulatory compliant(example: HIPAA)\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cspan style=\"color: #990000; font-size: x-large;\"\u003E\u003Cb\u003E3. Installation Prerequisites\u003C\/b\u003E\u003C\/span\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\n1.\u003Cspan style=\"white-space: pre;\"\u003E \u003C\/span\u003EDownload Stanford NER from \u003Ca href=\"http:\/\/nlp.stanford.edu\/software\/stanford-ner-2015-04-20.zip\"\u003Ehttp:\/\/nlp.stanford.edu\/software\/stanford-ner-2015-04-20.zip\u003C\/a\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n2.\u003Cspan style=\"white-space: pre;\"\u003E \u003C\/span\u003EUnzip the zipped folder and save in a drive.\u003C\/div\u003E\n\u003Cdiv\u003E\n3.\u003Cspan style=\"white-space: pre;\"\u003E \u003C\/span\u003ECopy the \u003Cb\u003E“stanford-ner.jar”\u003C\/b\u003E from the folder and save it just outside the folder as shown in the image\u003C\/div\u003E\n\u003Cdiv\u003E\n4.\u003Cspan style=\"white-space: pre;\"\u003E \u003C\/span\u003EDownload the caseless models from \u003Ca href=\"https:\/\/stanfordnlp.github.io\/CoreNLP\/history.html\"\u003Ehttps:\/\/stanfordnlp.github.io\/CoreNLP\/history.html\u003C\/a\u003E by clicking on “caseless” as given below. The models in the first link work as well. However, the caseless models help in identifying named entities even when they are not capitalised as required by formal grammar rules.\u0026nbsp;\u003C\/div\u003E\n\u003Cdiv\u003E\n5.\u003Cspan style=\"white-space: pre;\"\u003E \u003C\/span\u003ESave the folder in the same location as the Stanford NER folder for ease of access\u003C\/div\u003E\n\u003C\/div\u003E\n\u003Ctable cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"float: left; margin-right: 1em; text-align: left;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Ca href=\"https:\/\/3.bp.blogspot.com\/-KYKjb1q9mW4\/Ww1wRUrw3zI\/AAAAAAAAGbc\/4Ub2tFnyE1s5jlgmPP5rZGJr1cBauQEqQCLcBGAs\/s1600\/pic1.png\" imageanchor=\"1\" style=\"clear: left; margin-bottom: 1em; margin-left: auto; margin-right: auto;\"\u003E\u003Cimg border=\"0\" data-original-height=\"180\" data-original-width=\"1182\" height=\"96\" src=\"https:\/\/3.bp.blogspot.com\/-KYKjb1q9mW4\/Ww1wRUrw3zI\/AAAAAAAAGbc\/4Ub2tFnyE1s5jlgmPP5rZGJr1cBauQEqQCLcBGAs\/s640\/pic1.png\" width=\"640\" \/\u003E\u003C\/a\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003Ctr\u003E\u003Ctd class=\"tr-caption\" style=\"text-align: center;\"\u003EStanford NER Installation - Step1\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Ctable cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"float: left; margin-right: 1em; text-align: left;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Ca href=\"https:\/\/2.bp.blogspot.com\/-qsSTNVrI4wo\/Ww1wpTczvII\/AAAAAAAAGbk\/E8KEGJAastEKvVKJrUEan4hnhjWqUlvDACLcBGAs\/s1600\/pic2.PNG\" imageanchor=\"1\" style=\"clear: left; margin-bottom: 1em; margin-left: auto; margin-right: auto;\"\u003E\u003Cimg border=\"0\" data-original-height=\"176\" data-original-width=\"1027\" height=\"108\" src=\"https:\/\/2.bp.blogspot.com\/-qsSTNVrI4wo\/Ww1wpTczvII\/AAAAAAAAGbk\/E8KEGJAastEKvVKJrUEan4hnhjWqUlvDACLcBGAs\/s640\/pic2.PNG\" width=\"640\" \/\u003E\u003C\/a\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003Ctr\u003E\u003Ctd class=\"tr-caption\" style=\"text-align: center;\"\u003ENER Installation - Step2\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cdiv\u003E\n\u003Cspan style=\"color: #990000; font-size: large;\"\u003E\u003Cb\u003E4. Python Code for implementation:\u003C\/b\u003E\u003C\/span\u003E\u003C\/div\u003E\n\u003Cblockquote\u003E\n\u003Cdiv\u003E\n\u003Cb\u003E#Import all the required libraries.\u003C\/b\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\nimport os\u003C\/div\u003E\n\u003Cdiv\u003E\nfrom nltk.tag import StanfordNERTagger\u003C\/div\u003E\n\u003Cdiv\u003E\nimport pandas as pd\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cb\u003E#Set environmental variables programmatically.\u003C\/b\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cb\u003E#Set the classpath to the path where the jar file is located\u003C\/b\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\nos.environ['CLASSPATH'] = \"\u0026lt;path to the file\u0026gt;\/stanford-ner-2015-04-20\/stanford-ner.jar\"\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\n\u003Cb\u003E#Set the Stanford models to the path where the models are stored\u003C\/b\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\nos.environ['STANFORD_MODELS'] = '\u0026lt;path to the file\u0026gt;\/stanford-corenlp-caseless-2015-04-20-models\/edu\/stanford\/nlp\/models\/ner'\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cb\u003E\u003Cbr \/\u003E\u003C\/b\u003E\n\u003Cb\u003E#Set the java jdk path\u003C\/b\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\njava_path = \"C:\/Program Files\/Java\/jdk1.8.0_161\/bin\/java.exe\"\u003C\/div\u003E\n\u003Cdiv\u003E\nos.environ['JAVAHOME'] = java_path\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cb\u003E#Set the path to the model that you would like to use\u003C\/b\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\nstanford_classifier\u0026nbsp; =\u0026nbsp; '\u0026lt;path to the file\u0026gt;\/stanford-corenlp-caseless-2015-04-20-models\/edu\/stanford\/nlp\/models\/ner\/english.all.3class.caseless.distsim.crf.ser.gz'\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cb\u003E#Build NER tagger object\u003C\/b\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\nst = StanfordNERTagger(stanford_classifier)\u003C\/div\u003E\n\u003Cbr \/\u003E\n\u003Cdiv\u003E\n\u003Cb\u003E#A sample text for NER tagging\u003C\/b\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\ntext = 'srinivas ramanujan went to the united kingdom. There he studied at cambridge university.'\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cb\u003E#Tag the sentence and print output\u003C\/b\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\ntagged = st.tag(str(text).split())\u003C\/div\u003E\n\u003Cdiv\u003E\nprint(tagged)\u003C\/div\u003E\n\u003C\/blockquote\u003E\n\u003Cbr \/\u003E\n\u003Cspan style=\"font-size: large;\"\u003E\u003Cb\u003EOutput\u003C\/b\u003E\u003C\/span\u003E\u003Cbr \/\u003E\n\u003Cpre\u003E[(u'srinivas', u'PERSON'), \n(u'ramanujan', u'PERSON'), \n(u'went', u'O'), \n(u'to', u'O'), \n(u'the', u'O'), \n(u'united', u'LOCATION'), \n(u'kingdom.', u'LOCATION'), \n(u'There', u'O'), \n(u'he', u'O'), \n(u'studied', u'O'), \n(u'at', u'O'), \n(u'cambridge', u'ORGANIZATION'), \n(u'university', u'ORGANIZATION')]\n\u003C\/pre\u003E\n\u003Cbr \/\u003E\n\u003Cdiv\u003E\n\u003Cb\u003E\u003Cspan style=\"color: #990000; font-size: large;\"\u003E5. Additional Reading\u003C\/span\u003E\u003C\/b\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\nStanfordNER algorithm leverages a general implementation of linear chain Conditional Random Fields (CRFs) sequence models. CRFs seem very similar to Hidden Markov Model but are very different.\u003Cbr \/\u003E\n\u003Cbr \/\u003E\nBelow are some key points to note about the CRFs in general.\u003C\/div\u003E\n\u003Col style=\"text-align: left;\"\u003E\n\u003Cli\u003EIt is a discriminative model unlike the HMM model and thus models the conditional probability\u003C\/li\u003E\n\u003Cli\u003EIt does not assume independence of features unlike the HMM model. This means that the current word, previous word, next word are all considered for model as features\u003C\/li\u003E\n\u003Cli\u003ERelative to HMM or Max ent Markov Models, CRFs are the slowest\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003Cbr \/\u003E\n\u003Cdiv\u003E\n\u003Cb\u003E\u003Cspan style=\"color: #990000; font-size: large;\"\u003E6. Disclaimer\u003C\/span\u003E\u003C\/b\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cblockquote class=\"tr_bq\"\u003E\nThis article explains the implementation of StanfordNER algorithm for research purposes and does not promote it for commercial gains. For any questions on commercial aspects of implementing this algorithm, please contact Stanford University\u003C\/blockquote\u003E\n\u003C\/div\u003E\n\u003C\/div\u003E\n\u003C\/div\u003E\n\u003Cdiv class=\"blogger-post-footer\"\u003EThis post appeared first on \u003Ca href='https:\/\/www.listendata.com\/'\u003EListenData\u003C\/a\u003E\u003C\/div\u003E"},"link":[{"rel":"replies","type":"application/atom+xml","href":"https:\/\/www.listendata.com\/feeds\/569869026088308085\/comments\/default","title":"Post Comments"},{"rel":"replies","type":"text/html","href":"https:\/\/www.listendata.com\/2018\/05\/named-entity-recognition-using-python.html#comment-form","title":"8 Comments"},{"rel":"edit","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/569869026088308085"},{"rel":"self","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/569869026088308085"},{"rel":"alternate","type":"text/html","href":"https:\/\/www.listendata.com\/2018\/05\/named-entity-recognition-using-python.html","title":"Identify Person, Place and Organisation in content using Python"}],"author":[{"name":{"$t":"Unknown"},"email":{"$t":"noreply@blogger.com"},"gd$image":{"rel":"http://schemas.google.com/g/2005#thumbnail","width":"16","height":"16","src":"https:\/\/img1.blogblog.com\/img\/b16-rounded.gif"}}],"media$thumbnail":{"xmlns$media":"http://search.yahoo.com/mrss/","url":"https:\/\/1.bp.blogspot.com\/-ZpwUNrrzGDg\/Ww8BiYr0hjI\/AAAAAAAAHO4\/q77lafZmmqEUlLntQdKCwwVnnDx3IwAPQCLcBGAs\/s72-c\/nlp.png","height":"72","width":"72"},"thr$total":{"$t":"8"}},{"id":{"$t":"tag:blogger.com,1999:blog-7958828565254404797.post-2434660498940506110"},"published":{"$t":"2018-05-24T13:24:00.001-07:00"},"updated":{"$t":"2023-06-15T23:01:59.953-07:00"},"category":[{"scheme":"http://www.blogger.com/atom/ns#","term":"Data Science"},{"scheme":"http://www.blogger.com/atom/ns#","term":"Python"},{"scheme":"http://www.blogger.com/atom/ns#","term":"Text Mining"}],"title":{"type":"text","$t":"Case Study : Sentiment analysis using Python"},"content":{"type":"html","$t":"\u003Cdiv dir=\"ltr\" style=\"text-align: left;\" trbidi=\"on\"\u003E\nIn this article, we will walk you through an application of topic modelling and sentiment analysis to solve a real world business problem. This approach has a onetime effort of building a robust taxonomy and allows it to be regularly updated as new topics emerge. This approach is widely used in topic mapping tools. \u003Cb\u003EPlease note that this is not a replacement of the topic modelling methodologies such as Latent Dirichlet allocation (LDA) and it is beyond them.\u003C\/b\u003E\u003Cbr \/\u003E\n\u003Ctable align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Ca href=\"https:\/\/2.bp.blogspot.com\/-XhbqZTUDwNE\/WwcjP9obOiI\/AAAAAAAAHNk\/PL5i8Rgcs7sFYljcLKUyjH46tRIfVir4wCLcBGAs\/s1600\/sentiment_python.PNG\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"\u003E\u003Cimg border=\"0\" data-original-height=\"222\" data-original-width=\"457\" src=\"https:\/\/2.bp.blogspot.com\/-XhbqZTUDwNE\/WwcjP9obOiI\/AAAAAAAAHNk\/PL5i8Rgcs7sFYljcLKUyjH46tRIfVir4wCLcBGAs\/s1600\/sentiment_python.PNG\" \/\u003E\u003C\/a\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003Ctr\u003E\u003Ctd class=\"tr-caption\" style=\"text-align: center;\"\u003EText Mining Case Study using Python\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Cdiv class=\"MsoTitle\"\u003E\n\u003Ch2 style=\"text-align: left;\"\u003E\n\u003Cspan style=\"color: #990000; font-family: \u0026quot;arial\u0026quot; , \u0026quot;helvetica\u0026quot; , sans-serif; font-size: large;\"\u003E\u003Cb\u003E\u003Cbr \/\u003ECase Study : Topic Modeling and Sentiment Analysis\u003C\/b\u003E\u003C\/span\u003E\u003C\/h2\u003E\n\u003C\/div\u003E\nSuppose you are head of the analytics team with a leading Hotel chain “Tourist Hotel”. Each day, you receive hundreds of reviews of your hotel on the company’s website and multiple other social media pages. The business has a challenge of scale in analysing such data and identify areas of improvements. You use a taxonomy based approach to identify topics and then use a built-in functionality of Python NLTK package to attribute sentiment to the comments. This will help you in identifying what the customers like or dislike about your hotel.\u003Cbr \/\u003E\n\u003Cbr \/\u003E\n\u003Cdiv class=\"MsoTitle\"\u003E\n\u003Cspan style=\"color: #990000; font-family: \u0026quot;arial\u0026quot; , \u0026quot;helvetica\u0026quot; , sans-serif; font-size: large;\"\u003E\u003Cb\u003EData Structure\u003C\/b\u003E\u003C\/span\u003E\u003C\/div\u003E\n\u003Cdiv class=\"MsoTitle\"\u003E\n\u003Cbr \/\u003E\nThe customer review data consists of a serial number, an arbitrary identifier to identify each review uniquely and a text field that has the customer review.\u003Cbr \/\u003E\n\u003Ctable align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Cimg border=\"0\" data-original-height=\"94\" data-original-width=\"1013\" height=\"59\" src=\"https:\/\/1.bp.blogspot.com\/-m99mycfQGVs\/WwalrJmSkgI\/AAAAAAAAGaM\/3Jcq-b7dC08z2dkzTtZhdnNCt6Q3FAeqgCLcBGAs\/s640\/pic1.png\" style=\"margin-left: auto; margin-right: auto;\" width=\"640\" \/\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003Ctr\u003E\u003Ctd class=\"tr-caption\" style=\"text-align: center;\"\u003EExample : Sentiment Analysis\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\n\u003Cspan style=\"font-size: large;\"\u003E\u003Cb\u003ESteps to topic mapping and sentiment analysis\u003C\/b\u003E\u003C\/span\u003E\u003Cbr \/\u003E\n\u003Cbr \/\u003E\n1. Identify Topics and Sub Topics\u003Cbr \/\u003E\n2. Build Taxonomy\u003Cbr \/\u003E\n3. Map customer reviews to topics\u003Cbr \/\u003E\n4. Map customer reviews to sentiment\u003Cbr \/\u003E\n\u003Cbr \/\u003E\n\u003Cspan style=\"font-size: large;\"\u003E\u003Cb\u003EStep 1 : Identifying Topics\u003C\/b\u003E\u003C\/span\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\nThe first step is to identify the different topics in the reviews. You can use simple approaches such as Term Frequency and Inverse Document Frequency or more popular methodologies such as LDA to identify the topics in the reviews. In addition, it is a good practice to consult a subject matter expert in that domain to identify the common topics. For example, the topics in the “Tourist Hotel” example could be “Room booking”, “Room Price”, “Room Cleanliness”, “Staff Courtesy”, “Staff Availability ”etc.\u003Cbr \/\u003E\n\u003Cbr \/\u003E\n\u003Cdiv\u003E\n\u003Cspan style=\"font-size: large;\"\u003E\u003Cb\u003EStep 2 : Build Taxonomy\u003C\/b\u003E\u003C\/span\u003E\u003Cbr \/\u003E\n\u003Cb\u003E\u003Cbr \/\u003E\u003C\/b\u003E\n\u003Cb\u003EI. Build Topic Hierarchy\u003C\/b\u003E\u003C\/div\u003E\n\u003C\/div\u003E\n\u003Cdiv class=\"separator\" style=\"clear: both;\"\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv class=\"separator\" style=\"clear: both;\"\u003E\nBased on the topics from Step 1, Build a Taxonomy. A Taxonomy can be considered as a network of topics, sub topics and key words.\u003C\/div\u003E\n\u003Ctable align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Ca href=\"https:\/\/2.bp.blogspot.com\/-_eO3THeXyTE\/WwamBnoIV_I\/AAAAAAAAGaY\/RmXVaUrVMswiD8VqSExdyqkk9rk5yz2eACEwYBhgL\/s1600\/pic2.png\" imageanchor=\"1\" style=\"clear: left; display: inline !important; margin-bottom: 1em; margin-left: auto; margin-right: auto;\"\u003E\u003Cimg border=\"0\" data-original-height=\"281\" data-original-width=\"1188\" height=\"150\" src=\"https:\/\/2.bp.blogspot.com\/-_eO3THeXyTE\/WwamBnoIV_I\/AAAAAAAAGaY\/RmXVaUrVMswiD8VqSExdyqkk9rk5yz2eACEwYBhgL\/s640\/pic2.png\" width=\"640\" \/\u003E\u003C\/a\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003Ctr\u003E\u003Ctd class=\"tr-caption\" style=\"text-align: center;\"\u003ETopic Hierarchy\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Cb\u003EII. Build Keywords\u003C\/b\u003E\u003Cbr \/\u003E\nThe taxonomy is built in a CSV file format. There are 3 levels of key words for each sub topic namely, Primary key words, Additional key words and Exclude key words. The keywords for the topics need to be manually identified and added to the taxonomy file. The TfIDf, Bigram frequencies and LDA methodologies can help you in identifying the right set of keywords. Although there is no one best way for building key words, below is a suggested approach.\u003Cbr \/\u003E\n\u003Cbr \/\u003E\ni. Primary key words are the key words that are mostly specific to the topic. These key       words need to be mutually exclusive across different topics as far as possible.\u003Cbr \/\u003E\n\u003Cbr \/\u003E\nii. Additional key words are specific to the sub topic. These key words need not be mutually exclusive between the topics but it is advised to maintain exclusivity between sub topics under the same sub topic. To explain further, let us say, there is a sub topic “Price” under the topics “Room” as well as “Food”, then the additional key words will have an overlap. This will not create any issue as the primary key words are mutually exclusive. \u003Cbr \/\u003E\n\u003Cbr \/\u003E\niii. Exclude key words are key words that are used relatively less than the other two types. If there are two sub topics that have some overlap of additional words OR for example, if the sub topic “booking” is incorrectly mapping comments regarding taxi bookings as room booking, such key words could be used in exclude words to solve the problem.\u003Cbr \/\u003E\n\u003Cbr \/\u003E\n\u003Cdiv\u003E\n\u003Cdiv\u003E\n\u003Cspan style=\"font-family: \u0026quot;arial\u0026quot; , \u0026quot;helvetica\u0026quot; , sans-serif;\"\u003E\u003Cb\u003ESnapshot of sample taxonomy:\u003C\/b\u003E\u003C\/span\u003E\u003C\/div\u003E\n\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cspan style=\"font-family: \u0026quot;arial\u0026quot; , \u0026quot;helvetica\u0026quot; , sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Ctable align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Cimg border=\"0\" data-original-height=\"112\" data-original-width=\"1600\" height=\"44\" src=\"https:\/\/3.bp.blogspot.com\/-b6-807fneDk\/Wwan77MxawI\/AAAAAAAAGag\/UF8r0NV-iu4omQt-ISDvRTYoTK12vz-LgCLcBGAs\/s640\/pic3.png\" style=\"margin-left: auto; margin-right: auto;\" width=\"640\" \/\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003Ctr\u003E\u003Ctd class=\"tr-caption\" style=\"text-align: center;\"\u003ESample Taxonomy\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Ca href=\"https:\/\/3.bp.blogspot.com\/-b6-807fneDk\/Wwan77MxawI\/AAAAAAAAGag\/UF8r0NV-iu4omQt-ISDvRTYoTK12vz-LgCLcBGAs\/s1600\/pic3.png\" imageanchor=\"1\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;\"\u003E\u003Cspan style=\"font-family: \u0026quot;arial\u0026quot; , \u0026quot;helvetica\u0026quot; , sans-serif;\"\u003E\u003C\/span\u003E\u003C\/a\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cdiv\u003E\n\u003Ci\u003E\u003Cspan style=\"font-family: \u0026quot;arial\u0026quot; , \u0026quot;helvetica\u0026quot; , sans-serif;\"\u003ENote: while building the key word list, you can put an “*” at the end as it helps as wild character. For example, all the different inflections of “clean” such as “cleaned”, “cleanly”, “cleanliness” can be handled by one keyword “clean*”. If you need to add a phrase or any keyword with a special character in it, you can wrap it in quotes. For example, “online booking”, Wi-Fi” etc need to be in double quotes.\u003C\/span\u003E\u003C\/i\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\n\u003Cbr \/\u003E\n\u003Cspan style=\"color: #e69138; font-size: large;\"\u003E\u003Cb\u003EBenefits of using taxonomic\u0026nbsp;approach\u003C\/b\u003E\u003C\/span\u003E\u003Cbr \/\u003E\n\u003Cblockquote class=\"tr_bq\"\u003E\nTopic modelling approaches identify topics based on the keywords that are present in the content. For novel keywords that are similar to the topics but may come up in the future are not identified.\u0026nbsp;There could be use cases where businesses want to track certain topics that may not always be identified as topics by the topic modelling approaches.\u003C\/blockquote\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cspan style=\"font-size: large;\"\u003E\u003Cb\u003EStep 3 : Map customer reviews to topic\u003C\/b\u003E\u003C\/span\u003E\u003Cbr \/\u003E\n\u003Cbr \/\u003E\nEach customer comment is mapped to one or more sub topics. Some of the comments may not be mapped to any comment. Such instances need to be manually inspected to check if we missed any topics in the taxonomy so that it can be updated. Generally, about 90% of the comments have at least one topic. The rest of the comments could be vague. For example: “it was good experience” does not tell us anything specific and it is fine to leave it unmapped.\u003C\/div\u003E\n\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cspan style=\"font-family: \u0026quot;arial\u0026quot; , \u0026quot;helvetica\u0026quot; , sans-serif;\"\u003E\u003Cb\u003ESnapshot of how the topics are mapped:\u003C\/b\u003E\u003C\/span\u003E\u003C\/div\u003E\n\u003Ctable align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Cimg border=\"0\" data-original-height=\"208\" data-original-width=\"1024\" height=\"130\" src=\"https:\/\/4.bp.blogspot.com\/-a_6eGD7yI-U\/Wwaocx0ooGI\/AAAAAAAAGao\/2P4iKJycr1M7ATWP0lGrxpHRN8BQpZS2ACLcBGAs\/s640\/pic4.png\" style=\"margin-left: auto; margin-right: auto;\" width=\"640\" \/\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003Ctr\u003E\u003Ctd class=\"tr-caption\" style=\"text-align: center;\"\u003ETopic Mapping\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Cdiv\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\nBelow is the python code that helps in mapping reviews to categories. Firstly, import all the libraries needed for this task. Install these libraries if needed.\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cblockquote class=\"tr_bq\"\u003E\nimport pandas as pd\u003Cbr \/\u003E\nimport numpy as np\u003Cbr \/\u003E\nimport re\u003Cbr \/\u003E\nimport string\u003Cbr \/\u003E\nimport nltk\u003Cbr \/\u003E\nfrom nltk.tokenize import word_tokenize\u003Cbr \/\u003E\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\u003C\/blockquote\u003E\n\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\n\u003Cb\u003E\u003Cspan style=\"color: #b45f06; font-size: large;\"\u003EDownload Datafiles\u003C\/span\u003E\u003C\/b\u003E\u003Cbr \/\u003E\n\u003Ca href=\"https:\/\/github.com\/deepanshu88\/Datasets\/raw\/master\/UploadedFiles2\/customer_reviews.csv\" target=\"_blank\"\u003E\u003Cb\u003ECustomer Review\u003C\/b\u003E\u003C\/a\u003E\u003Cbr \/\u003E\n\u003Ca href=\"https:\/\/github.com\/deepanshu88\/Datasets\/raw\/master\/UploadedFiles2\/taxonomy.csv\" target=\"_blank\"\u003E\u003Cb\u003ETaxonomy\u003C\/b\u003E\u003C\/a\u003E\u003Cbr \/\u003E\n\u003Cbr \/\u003E\n\u003Cb\u003E\u003Cspan style=\"color: #b45f06; font-size: large;\"\u003EDownload Python Code\u003C\/span\u003E\u003C\/b\u003E\u003Cbr \/\u003E\n\u003Cdiv\u003E\nIf you copy-paste the code from the article, some of the lines of code might not work as python follows indentation very strictly so download python code from the link below.\u0026nbsp; \u003Cb\u003EThe code is built in Python 2.7\u003C\/b\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cb\u003E\u003Ca href=\"https:\/\/github.com\/deepanshu88\/Datasets\/raw\/master\/UploadedFiles2\/CaSE.py\" target=\"_blank\"\u003EPython - Sentiment Analysis\u003C\/a\u003E\u003C\/b\u003E\u003C\/div\u003E\n\u003Cbr \/\u003E\n\u003Cb\u003EImport reviews data\u003C\/b\u003E\u003Cbr \/\u003E\n\u003Cblockquote class=\"tr_bq\"\u003E\ndf = pd.read_csv(\"D:\/customer_reviews.csv\");\u003C\/blockquote\u003E\n\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cb\u003EImport taxonomy\u003C\/b\u003E\u003Cbr \/\u003E\n\u003Cblockquote class=\"tr_bq\"\u003E\ndf_tx = pd.read_csv(\"D:\/ taxonomy.csv\");\u003C\/blockquote\u003E\n\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\nBuild functions for handling the various repetitive tasks during the mapping exercise. \u003Cb\u003EThis function identifies taxonomy words ending with (*) and treats it as a wild character. \u003C\/b\u003EThis takes the Keywords as input and uses regular expression to identify all the other keyword matches as output.\u003Cbr \/\u003E\n\u003Cpre\u003Edef asterix_handler(asterixw, lookupw):\n    mtch = \"F\"\n    for word in asterixw:\n        for lword in lookupw:\n            if(word[-1:]==\"*\"):\n                if(bool(re.search(\"^\"+ word[:-1],lword))==True):\n                    mtch = \"T\"\n                    break\n    return(mtch)\n\u003C\/pre\u003E\n\u003Cb\u003EThis function removes all punctuations. \u003C\/b\u003EThis is helpful in terms of data cleaning. You can edit the list of punctuations for your own custom punctuation removal at the place highlighted in amber.\u003C\/div\u003E\n\u003Cpre\u003Edef remov_punct(withpunct):\n    punctuations = '''!()-[]{};:'\"\\,\u0026lt;\u0026gt;.\/?@#$%^\u0026amp;*_~'''\n    without_punct = \"\"\n    char = 'nan'\n    for char in withpunct:\n        if char not in punctuations:\n            without_punct = without_punct + char\n    return(without_punct)\n\u003C\/pre\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\n\u003Cb\u003EFunction to remove just the quotes(\"\").\u003C\/b\u003E This is different from the above as this only handles double quotes. Recall that we wrap phrases or key words with special characters in double quotes.\u003Cbr \/\u003E\n\u003Cpre\u003Edef remov_quote(withquote):\n    quote = '\"'\n    without_quote = \"\"\n    char = 'nan'\n    for char in withquote:\n        if char not in quote:\n            without_quote = without_quote + char\n    return(without_quote)        \n\u003C\/pre\u003E\n\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\nSplit each document by sentences and append one below the other for sentence level topic mapping.\u003Cbr \/\u003E\n\u003Cpre\u003Esentence_data = pd.DataFrame(columns=['slno','text'])\nfor d in range(len(df)):    \n    doc = (df.iloc[d,1].split('.'))\n    for s in ((doc)):        \n        temp = {'slno': [df['slno'][d]], 'text': [s]}\n        sentence_data =  pd.concat([sentence_data,pd.DataFrame(temp)])\n        temp = \"\"\n\u003C\/pre\u003E\n\u003Cb\u003EDrop empty text rows if any and export data\u003C\/b\u003E\n\u003Cbr \/\u003E\n\u003Cpre\u003Esentence_data['text'].replace('',np.nan,inplace=True);      \nsentence_data.dropna(subset=['text'], inplace=True);  \n\n\ndata = sentence_data\ncat2list = list(set(df_tx['Subtopic']))\ndata['Category'] = 0\nmapped_data = pd.DataFrame(columns = ['slno','text','Category']);\ntemp=pd.DataFrame()\nfor k in range(len(data)):\n    comment = remov_punct(data.iloc[k,1])\n    data_words = [str(x.strip()).lower() for x in str(comment).split()]\n    data_words = filter(None, data_words)\n    output = []\n    \n    for l in range(len(df_tx)):\n        key_flag = False\n        and_flag = False\n        not_flag = False\n        if (str(df_tx['PrimaryKeywords'][l])!='nan'):\n            kw_clean = (remov_quote(df_tx['PrimaryKeywords'][l]))\n        if (str(df_tx['AdditionalKeywords'][l])!='nan'):\n            aw_clean = (remov_quote(df_tx['AdditionalKeywords'][l]))\n        else:\n            aw_clean = df_tx['AdditionalKeywords'][l]\n        if (str(df_tx['ExcludeKeywords'][l])!='nan'):\n            nw_clean = remov_quote(df_tx['ExcludeKeywords'][l])\n        else:\n            nw_clean = df_tx['ExcludeKeywords'][l]\n        Key_words = 'nan'\n        and_words = 'nan'\n        and_words2 = 'nan'\n        not_words = 'nan'\n        not_words2 = 'nan'\n        \n        if(str(kw_clean)!='nan'):\n            key_words = [str(x.strip()).lower() for x in kw_clean.split(',')]\n            key_words2 = set(w.lower() for w in key_words)\n        \n        if(str(aw_clean)!='nan'):\n            and_words = [str(x.strip()).lower() for x in aw_clean.split(',')]\n            and_words2 = set(w.lower() for w in and_words)\n        \n        if(str(nw_clean)!= 'nan'):\n            not_words = [str(x.strip()).lower() for x in nw_clean.split(',')]\n            not_words2 = set(w.lower() for w in not_words)\n        \n        if(str(kw_clean) == 'nan'):\n            key_flag = False        \n        else:\n            if set(data_words) \u0026amp; key_words2:\n                key_flag = True\n            else:\n                if(asterix_handler(key_words2, data_words)=='T'):\n                    key_flag = True\n                    \n        if(str(aw_clean)=='nan'):\n            and_flag = True\n        else:\n            if set(data_words) \u0026amp; and_words2:\n                and_flag = True\n            else:\n                if(asterix_handler(and_words2,data_words)=='T'):\n                    and_flag = True\n        if(str(nw_clean) == 'nan'):\n            not_flag = False\n        else:\n            if set(data_words) \u0026amp; not_words2:\n                not_flag = True\n            else:\n                if(asterix_handler(not_words2, data_words)=='T'):\n                    not_flag = True\n        if(key_flag == True and and_flag == True and not_flag == False):\n            output.append(str(df_tx['Subtopic'][l]))            \n            temp = {'slno': [data.iloc[k,0]], 'text': [data.iloc[k,1]], 'Category': [df_tx['Subtopic'][l]]}\n            mapped_data = pd.concat([mapped_data,pd.DataFrame(temp)])\n    #data['Category'][k] = ','.join(output)\n#output mapped data\nmapped_data.to_csv(\"D:\/mapped_data.csv\",index = False)\n\u003C\/pre\u003E\n\u003C\/div\u003E\n\u003Cbr \/\u003E\n\u003Cb\u003EStep 4: Map customer reviews to sentiment\u003C\/b\u003E\u003Cbr \/\u003E\n\u003Cblockquote class=\"tr_bq\"\u003E\n#read category mapped data for sentiment mapping\u003Cbr \/\u003E\ncatdata = pd.read_csv(\"D:\/mapped_data.csv\")\u003C\/blockquote\u003E\n#Build a function to leverage the built-in NLTK functionality of identifying sentiment. The output 1 means positive, 0 means neutral and -1 means negative. You can choose your own set of thresholds for positive, neutral and negative sentiment.\u003Cbr \/\u003E\n\u003Cbr \/\u003E\n\u003Cdiv\u003E\n\u003Cpre\u003Edef findpolar(test_data):\n    sia = SentimentIntensityAnalyzer()\n    polarity = sia.polarity_scores(test_data)[\"compound\"];\n    if(polarity \u0026gt;= 0.1):    \n     foundpolar = 1\n    if(polarity \u0026lt;= -0.1):\n     foundpolar = -1 \n    if(polarity\u0026gt;= -0.1 and polarity\u0026lt;= 0.1):\n     foundpolar = 0\n     \n    return(foundpolar)\n\u003C\/pre\u003E\n\u003Cb\u003E\u003Cbr \/\u003E\u003C\/b\u003E\n\u003Cb\u003EOutput the sentiment mapped data\u003C\/b\u003E\n\u003Cbr \/\u003E\n\u003Cblockquote\u003E\ncatdata.to_csv(\"D:\/sentiment_mapped_data.csv\",index = False)\u003C\/blockquote\u003E\n\u003Cdiv\u003E\n\u003Ctable align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Ca href=\"https:\/\/2.bp.blogspot.com\/-qqAl6lDCl3M\/Wwao8-Zoh1I\/AAAAAAAAGa0\/GlYMBdhlQrUqSs2BrpQYkD0bVcX8cdi4QCLcBGAs\/s1600\/pic5.png\" imageanchor=\"1\" style=\"clear: left; display: inline !important; margin-bottom: 1em; margin-left: auto; margin-right: auto; text-align: center;\"\u003E\u003Cimg border=\"0\" data-original-height=\"88\" data-original-width=\"829\" height=\"66\" src=\"https:\/\/2.bp.blogspot.com\/-qqAl6lDCl3M\/Wwao8-Zoh1I\/AAAAAAAAGa0\/GlYMBdhlQrUqSs2BrpQYkD0bVcX8cdi4QCLcBGAs\/s640\/pic5.png\" width=\"640\" \/\u003E\u003C\/a\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003Ctr\u003E\u003Ctd class=\"tr-caption\" style=\"text-align: center;\"\u003EOutput : Sentiment Analysis\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cdiv\u003E\n\u003Cspan style=\"font-family: \u0026quot;arial\u0026quot; , \u0026quot;helvetica\u0026quot; , sans-serif;\"\u003E\u003Cb\u003E\u003Cu\u003EAdditional Reading\u003C\/u\u003E\u003C\/b\u003E\u003C\/span\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cspan style=\"font-family: \u0026quot;arial\u0026quot; , \u0026quot;helvetica\u0026quot; , sans-serif;\"\u003E\u003Cb\u003E\u003Cbr \/\u003E\u003C\/b\u003E\u003C\/span\u003E\n\u003Cspan style=\"font-family: \u0026quot;arial\u0026quot; , \u0026quot;helvetica\u0026quot; , sans-serif;\"\u003E\u003Cb\u003EPolarity Scoring Explained:\u0026nbsp;\u003C\/b\u003E\u003C\/span\u003E\u003Cbr \/\u003E\n\u003Cbr \/\u003E\nNLTK offers Valence Aware Dictionary for sEntiment Reasoning(VADER)  model that helps in identifying both the direction (polarity) as well as the magnitude(intensity) of the text. Below is the high-level explanation of the methodology.\u003Cbr \/\u003E\n\u003Cbr \/\u003E\nVADER is a combination of lexical features and rules to identify sentiment and intensity. Hence, this does not need any training data.\u0026nbsp; To explain further, if we take an example of the sentence “the food is good”, it is easy to identify that it is positive in sentiment. VADER goes a step ahead and identifies intensity based on rule based approach such as punctuation, capitalised words and degree modifications.\u003Cbr \/\u003E\n\u003Cbr \/\u003E\nThe polarity scores for the different variations of similar sentences is as follows:\u003Cbr \/\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003Ctable align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Cimg border=\"0\" data-original-height=\"181\" data-original-width=\"669\" height=\"107\" src=\"https:\/\/4.bp.blogspot.com\/-w4PQ3QPJ1Ws\/WwapnseZARI\/AAAAAAAAGa8\/qi6zJboasL4UUGT6eL5aImMyCmPVRm50QCLcBGAs\/s400\/pic6.PNG\" style=\"margin-left: auto; margin-right: auto;\" width=\"400\" \/\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003Ctr\u003E\u003Ctd class=\"tr-caption\" style=\"text-align: center;\"\u003EPolarity Score\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Cdiv class=\"separator\" style=\"clear: both; text-align: center;\"\u003E\n\u003Cspan style=\"font-family: \u0026quot;arial\u0026quot; , \u0026quot;helvetica\u0026quot; , sans-serif; margin-left: 1em; margin-right: 1em;\"\u003E\u003C\/span\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\u003C\/div\u003E\n\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cb\u003EUse cases where training sentiment models is suggested over Sentiment Intensity Analyzer:\u003C\/b\u003E\u003C\/div\u003E\n\u003Cdiv\u003E\n\u003Cbr \/\u003E\nAlthough VADER works well on multiple domains, there are could be some domains where it is preferred to build one’s own sentiment training models. Below are the two examples of such use cases.\u003Cbr \/\u003E\n\u003Col style=\"text-align: left;\"\u003E\n\u003Cli\u003E\u003Cb\u003ECustomer reviews on alcoholic beverages:\u003C\/b\u003E\u003C\/li\u003E\nIt is common to observe people using otherwise negative sentiment words to describe positive experience. For example, the sentence “this sh*t is fu**ing good” means that this drink is good but VADER approach gives it a “-10” suggesting negative sentiment\u003Cbr \/\u003E\n\u003Cbr \/\u003E\n\u003Cli\u003E\u003Cb\u003EPatient reviews regarding hospital treatment\u003C\/b\u003E\u003C\/li\u003E\nPatient’s description of their problem is a neutral sentiment but VADER approach considers it as negative sentiment. For example, the sentence “I had an unbearable back pain and your medication cured me in no time” is given “-0.67” suggesting negative sentiment.\u003C\/ol\u003E\n\u003C\/div\u003E\n\u003C\/div\u003E\n\u003C\/div\u003E\n\u003C\/div\u003E\u003Cdiv class=\"blogger-post-footer\"\u003EThis post appeared first on \u003Ca href='https:\/\/www.listendata.com\/'\u003EListenData\u003C\/a\u003E\u003C\/div\u003E"},"link":[{"rel":"replies","type":"application/atom+xml","href":"https:\/\/www.listendata.com\/feeds\/2434660498940506110\/comments\/default","title":"Post Comments"},{"rel":"replies","type":"text/html","href":"https:\/\/www.listendata.com\/2018\/05\/sentiment-analysis-using-python.html#comment-form","title":"8 Comments"},{"rel":"edit","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/2434660498940506110"},{"rel":"self","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/2434660498940506110"},{"rel":"alternate","type":"text/html","href":"https:\/\/www.listendata.com\/2018\/05\/sentiment-analysis-using-python.html","title":"Case Study : Sentiment analysis using Python"}],"author":[{"name":{"$t":"Unknown"},"email":{"$t":"noreply@blogger.com"},"gd$image":{"rel":"http://schemas.google.com/g/2005#thumbnail","width":"16","height":"16","src":"https:\/\/img1.blogblog.com\/img\/b16-rounded.gif"}}],"media$thumbnail":{"xmlns$media":"http://search.yahoo.com/mrss/","url":"https:\/\/2.bp.blogspot.com\/-XhbqZTUDwNE\/WwcjP9obOiI\/AAAAAAAAHNk\/PL5i8Rgcs7sFYljcLKUyjH46tRIfVir4wCLcBGAs\/s72-c\/sentiment_python.PNG","height":"72","width":"72"},"thr$total":{"$t":"8"}},{"id":{"$t":"tag:blogger.com,1999:blog-7958828565254404797.post-5116849402452216087"},"published":{"$t":"2018-03-25T11:50:00.004-07:00"},"updated":{"$t":"2024-02-13T06:27:06.237-08:00"},"category":[{"scheme":"http://www.blogger.com/atom/ns#","term":"Data Science"},{"scheme":"http://www.blogger.com/atom/ns#","term":"R"},{"scheme":"http://www.blogger.com/atom/ns#","term":"regression"}],"title":{"type":"text","$t":"15 Types of Regression (with Examples)"},"content":{"type":"html","$t":"\u003Cp\u003ERegression methods are widely used for predictive modeling. Most analytics professionals are familiar with only 2-3 common types such as linear and logistic regression. However, there are over 10 regression algorithms designed for different data and analyses. Understanding the right regression type based on data and distribution is important for effective analysis.\u003C\/p\u003E\n\u003Cdiv id=\"toc\"\u003E\n\u003Cspan\u003ETable of Contents\u003C\/span\u003E\u003C\/div\u003E\n\u003Chr \/\u003E\n\u003Cdiv id=\"contents\"\u003E\n\u003Cimg alt=\"Regression Analysis\" data-original-height=\"305\" data-original-width=\"610\" src=\"https:\/\/4.bp.blogspot.com\/-fBXCbdE9PLg\/XP5nZmMTzfI\/AAAAAAAAHnU\/w3c-fAwT_u81HGQv0_bf794mwaYMK7RYQCLcBGAs\/s1600\/regression_R.PNG\" \/\u003E\n\u003Ch2 class='top2'\u003EWhat is Regression Analysis?\u003C\/h2\u003E\n\u003Cp\u003EIn simple words, regression analysis is used to model the relationship between a dependent variable and one or more independent variables.\u003C\/p\u003E\n\u003Cp\u003E\u003Cb\u003ELets take a simple example :\u003C\/b\u003E Suppose your manager asked you to predict annual sales. There can be a hundred of factors (drivers) that affects sales. In this case, sales is your \u003Cb\u003Edependent variable\u003C\/b\u003E. Factors affecting sales are \u003Cb\u003Eindependent variables\u003C\/b\u003E. Regression analysis would help you to solve this problem.\u003C\/p\u003E\n\u003Cp\u003EIt helps us to answer the following questions -\u003C\/p\u003E\n\u003Col\u003E\n\u003Cli\u003EWhich of the drivers have a significant impact on sales\u003C\/li\u003E\n\u003Cli\u003EWhich is the most important driver of sales\u003C\/li\u003E\n\u003Cli\u003EHow do the drivers interact with each other\u003C\/li\u003E\n\u003Cli\u003EWhat would be the annual sales next year.\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003Ch2 class='top2'\u003ETerminologies related to regression analysis\u003C\/h2\u003E\n\u003Cdiv class=\"db2\"\u003E1. Outliers\u003C\/div\u003E\n\u003Cp\u003ESuppose there is an observation in the dataset which is having a very high or very low value as compared to the other observations in the data, i.e. it does not belong to the population, such an observation is called an outlier. In simple words, it is extreme value. An outlier is a problem because many times it hampers the results we get.\u003C\/p\u003E\n\u003Cdiv class=\"db2\"\u003E2. Multicollinearity\u003C\/div\u003E\n\u003Cp\u003EWhen the independent variables are highly correlated to each other then the variables are said to be multicollinear. Many types of regression techniques assumes multicollinearity should not be present in the dataset. It is because it causes problems in ranking variables based on its importance. Or it makes job difficult in selecting the most important independent variable (factor).\u003C\/p\u003E\n\u003Cdiv class=\"db2\"\u003E3. Heteroscedasticity\u003C\/div\u003E\n\u003Cp\u003EWhen dependent variable's variability is not equal across values of an independent variable, it is called heteroscedasticity.\u003C\/p\u003E\n\u003Cp\u003E\u003Cb\u003EExample -\u003C\/b\u003EAs one's income increases, the variability of food consumption will increase. A poorer person will spend a rather constant amount by always eating inexpensive food; a wealthier person may occasionally buy inexpensive food and at other times eat expensive meals. Those with higher incomes display a greater variability of food consumption.\u003C\/p\u003E\n\u003Cdiv class=\"db2\"\u003E4. Underfitting and Overfitting\u003C\/div\u003E\n\u003Cp\u003EWhen we use unnecessary explanatory variables it might lead to overfitting. Overfitting means that our algorithm works well on the training set but is unable to perform better on the test sets. It is also known as problem of \u003Cb\u003Ehigh variance.\u003C\/b\u003E\n\u003C\/p\u003E\n\u003Cp\u003EWhen our algorithm works so poorly that it is unable to fit even training set well then it is said to \u003Cb\u003Eunderfit the data.\u003C\/b\u003EIt is also known as \u003Cb\u003Eproblem of high bias.\u003C\/b\u003E\n\u003C\/p\u003E\n\u003Cp\u003EIn the following diagram we can see that fitting a linear regression (straight line in fig 1) would underfit the data i.e. it will lead to large errors even in the training set. Using a polynomial fit in fig 2 is balanced i.e. such a fit can work on the training and test sets well, while in fig 3 the fit will lead to low errors in training set but it will not work well on the test set.\n\u003C\/p\u003E\n\u003Ctable align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Ca href=\"https:\/\/4.bp.blogspot.com\/-dM4Iae3kVsQ\/Wlt28eEHHiI\/AAAAAAAACPg\/X0dIT2a6RMwdEFUO44fQVX9HXakraYBagCLcBGAs\/s1600\/img1.png\" style=\"margin-left: auto; margin-right: auto;\"\u003E\u003Cimg alt=\"Underfitting vs Overfitting\" border=\"0\" data-original-height=\"216\" data-original-width=\"715\" height=\"192\" src=\"https:\/\/4.bp.blogspot.com\/-dM4Iae3kVsQ\/Wlt28eEHHiI\/AAAAAAAACPg\/X0dIT2a6RMwdEFUO44fQVX9HXakraYBagCLcBGAs\/s640\/img1.png\" title=\"\" width=\"640\" \/\u003E\u003C\/a\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003Ctr\u003E\u003Ctd class=\"tr-caption\" style=\"text-align: center;\"\u003ERegression : Underfitting and Overfitting\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Ch2 class='top2'\u003ETypes of Regression\u003C\/h2\u003E\n\u003Cp\u003EEvery regression technique has some assumptions attached to it which we need to meet before running analysis. These techniques differ in terms of type of dependent and independent variables and distribution.\u003C\/p\u003E\n\u003Ch3\u003E1. Linear Regression\u003C\/h3\u003E\n\u003Cp\u003EIt is the simplest form of regression. It is a technique in which the \u003Cb\u003Edependent variable is continuous\u003C\/b\u003E in nature. The relationship between the dependent variable and independent variables is assumed to be linear in nature.We can observe that the given plot represents a somehow linear relationship between the mileage and displacement of cars. The \u003Cb\u003Egreen points\u003C\/b\u003E are the actual observations while the \u003Cb\u003Eblack line fitted\u003C\/b\u003E is the line of regression.\u003C\/p\u003E\n\u003Ctable align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Ca href=\"https:\/\/4.bp.blogspot.com\/-IOOxgPaXMVc\/Wlj3LWvcnjI\/AAAAAAAACKE\/UeTFYvAxDmUDel5UBjdifeWaApB3-dXVgCLcBGAs\/s1600\/img1.jpg\" style=\"margin-left: auto; margin-right: auto;\"\u003E\u003Cimg alt=\"Linear Regression analysis\" border=\"0\" data-original-height=\"299\" data-original-width=\"496\" src=\"https:\/\/4.bp.blogspot.com\/-IOOxgPaXMVc\/Wlj3LWvcnjI\/AAAAAAAACKE\/UeTFYvAxDmUDel5UBjdifeWaApB3-dXVgCLcBGAs\/s1600\/img1.jpg\" title=\"\" \/\u003E\u003C\/a\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003Ctr\u003E\u003Ctd class=\"tr-caption\" style=\"text-align: center;\"\u003ERegression Analysis\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Cp\u003EWhen you have \u003Ccode\u003Eonly 1 independent variable\u003C\/code\u003E and 1 dependent variable, it is called simple linear regression. When you have \u003Ccode\u003Emore than 1 independent variable\u003C\/code\u003E and 1 dependent variable, it is called Multiple linear regression.\u003C\/p\u003E\n\u003Cdiv class=\"db2\"\u003EThe equation of multiple linear regression is listed below -\u003C\/div\u003E\n\u003Ctable align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Ca href=\"https:\/\/2.bp.blogspot.com\/-xbqTM5K3bIU\/WkzhtHMPEmI\/AAAAAAAACFs\/RULnlMKw_0U14oRWOUcuETJNt9TBYiJEgCLcBGAs\/s1600\/b.jpg\" style=\"margin-left: auto; margin-right: auto;\"\u003E\n\u003Cimg alt=\"equation of multiple linear regression\" data-original-height=\"44\" data-original-width=\"320\" src=\"https:\/\/2.bp.blogspot.com\/-xbqTM5K3bIU\/WkzhtHMPEmI\/AAAAAAAACFs\/RULnlMKw_0U14oRWOUcuETJNt9TBYiJEgCLcBGAs\/s1600\/b.jpg\" \/\u003E\u003C\/a\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Cp\u003EHere 'y' is the dependent variable to be estimated, and X are the independent variables and ε is the error term. βi’s are the regression coefficients.\n\u003C\/p\u003E\n\u003Cdiv class=\"db2\"\u003EAssumptions of linear regression:\u003C\/div\u003E\n\u003Col\u003E\n\u003Cli\u003EThere must be a linear relation between independent and dependent variables.\u003C\/li\u003E\n\u003Cli\u003EThere should not be any outliers present.\u003C\/li\u003E\n\u003Cli\u003ENo heteroscedasticity\u003C\/li\u003E\n\u003Cli\u003ESample observations should be independent.\u003C\/li\u003E\n\u003Cli\u003EError terms should be normally distributed with mean 0 and constant variance.\u003C\/li\u003E\n\u003Cli\u003EAbsence of multicollinearity and auto-correlation.\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003Cdiv class=\"db2\"\u003EEstimating the parameters\u003C\/div\u003E\n\u003Cp\u003ETo estimate the regression coefficients βi’s we use principle of least squares which is to minimize the sum of squares due to the error terms i.e. \u003C\/p\u003E\n\u003Ca href=\"https:\/\/3.bp.blogspot.com\/-bHdTkTHhk-A\/Wlj7qArK-vI\/AAAAAAAACKQ\/Afedqlb4p1AFVg9MO623FbdUhZKmIeFXACLcBGAs\/s1600\/img2.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\n\u003Cimg alt=\"estimate the regression coefficients\" data-original-height=\"38\" data-original-width=\"332\" height=\"43\" src=\"https:\/\/3.bp.blogspot.com\/-bHdTkTHhk-A\/Wlj7qArK-vI\/AAAAAAAACKQ\/Afedqlb4p1AFVg9MO623FbdUhZKmIeFXACLcBGAs\/s400\/img2.jpg\" width=\"400\" \/\u003E\u003C\/a\u003E\n\u003Cp\u003EOn solving the above equation mathematically we obtain the regression coefficients as:\u003C\/p\u003E\n\u003Cspan style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Ca href=\"https:\/\/4.bp.blogspot.com\/-Srjys9kedH8\/Wlj8XpIW4dI\/AAAAAAAACKY\/KeuZNb4RZpkoKrtFmpoDbq07ZXeKBvI1wCLcBGAs\/s1600\/img3.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\n\u003Cimg alt=\"Regression coefficients\" data-original-height=\"35\" data-original-width=\"135\" height=\"51\" src=\"https:\/\/4.bp.blogspot.com\/-Srjys9kedH8\/Wlj8XpIW4dI\/AAAAAAAACKY\/KeuZNb4RZpkoKrtFmpoDbq07ZXeKBvI1wCLcBGAs\/s200\/img3.jpg\" width=\"200\" \/\u003E\u003C\/a\u003E\u003C\/span\u003E\n\u003Cdiv class=\"db2\"\u003EInterpretation of regression coefficients\u003C\/div\u003E\n\u003Cp\u003ELet us consider an example where the dependent variable is marks obtained by a student and explanatory variables are number of hours studied and no. of classes attended. Suppose on fitting linear regression we got the linear regression as:\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003EMarks obtained = 5 + 2 (no. of hours studied) + 0.5(no. of classes attended)\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EThus we can have the regression coefficients 2 and 0.5 which can interpreted as:\u003C\/p\u003E\n\u003Col\u003E\n\u003Cli\u003EIf no. of hours studied and no. of classes are 0 then the student will obtain 5 marks.\u003C\/li\u003E\n\u003Cli\u003EKeeping no. of classes attended constant, if student studies for one hour more then he will score 2 more marks in the examination.\u003C\/li\u003E\n\u003Cli\u003ESimilarly keeping no. of hours studied constant, if student attends one more class then he will attain 0.5 marks more.\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003Cdiv class=\"db2\"\u003ELinear Regression in R\u003C\/div\u003E\n\u003Cp\u003EWe consider the swiss data set for carrying out linear regression in R. We use lm() function in the base package. We try to estimate Fertility with the help of other variables.\n\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\nlibrary(datasets)\nmodel = lm(Fertility ~ .,data = swiss)\nlm_coeff = model$coefficients\nlm_coeff\nsummary(model)\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cdiv class='db2'\u003EOutput\u003C\/div\u003E \n\u003Cpre\u003E\u003Ccode\u003E     (Intercept)      Agriculture      Examination        Education         Catholic \n      66.9151817       -0.1721140       -0.2580082       -0.8709401        0.1041153 \nInfant.Mortality \n       1.0770481 \n\n\nCall:\nlm(formula = Fertility ~ ., data = swiss)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2743  -5.2617   0.5032   4.1198  15.3213 \nCoefficients:\n                 Estimate Std. Error t value Pr(\u0026gt;|t|)    \n(Intercept)      66.91518   10.70604   6.250 1.91e-07 ***\nAgriculture      -0.17211    0.07030  -2.448  0.01873 *  \nExamination      -0.25801    0.25388  -1.016  0.31546    \nEducation        -0.87094    0.18303  -4.758 2.43e-05 ***\nCatholic          0.10412    0.03526   2.953  0.00519 ** \nInfant.Mortality  1.07705    0.38172   2.822  0.00734 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\nResidual standard error: 7.165 on 41 degrees of freedom\nMultiple R-squared:  0.7067, Adjusted R-squared:  0.671 \nF-statistic: 19.76 on 5 and 41 DF,  p-value: 5.594e-10\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EHence we can see that 70% of the variation in Fertility rate can be explained via linear regression.\n\u003C\/p\u003E\n\u003Ch3\u003E2. Polynomial Regression\u003C\/h3\u003E\n\u003Cp\u003EIt is a technique to fit a nonlinear equation by taking polynomial functions of independent variable.\u003C\/p\u003E\n\u003Cp\u003EIn the figure given below, you can see the red curve fits the data better than the green curve. Hence in the situations where the relation between the dependent and independent variable seems to be non-linear we can deploy \u003Cb\u003EPolynomial Regression Models.\u003C\/b\u003E\u003Cbr\/\u003E\n\u003C\/p\u003E\n\u003Ca href=\"https:\/\/1.bp.blogspot.com\/-dODuK8N5h1Q\/Wlnyb3V9HFI\/AAAAAAAACL4\/WxQtCJ1pM5wccDABg4wIrTBUB0vlikXQQCLcBGAs\/s1600\/poly1.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Cimg alt=\"Polynomial Regression\" data-original-height=\"258\" data-original-width=\"469\" src=\"https:\/\/1.bp.blogspot.com\/-dODuK8N5h1Q\/Wlnyb3V9HFI\/AAAAAAAACL4\/WxQtCJ1pM5wccDABg4wIrTBUB0vlikXQQCLcBGAs\/s1600\/poly1.jpg\" \/\u003E\u003C\/a\u003E\n\u003Cp\u003EThus a polynomial of degree k in one variable is written as:\u003C\/p\u003E\n\u003Ca href=\"https:\/\/1.bp.blogspot.com\/-wrJdHn0X_Y8\/Wln1K2YZO5I\/AAAAAAAACMI\/gScVjBesYCY0S4bqUV_tVL6DELUjVcvLwCLcBGAs\/s1600\/poly2.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\n\u003Cimg alt=\"polynomial of degree k in variable\" data-original-height=\"43\" data-original-width=\"242\" src=\"https:\/\/1.bp.blogspot.com\/-wrJdHn0X_Y8\/Wln1K2YZO5I\/AAAAAAAACMI\/gScVjBesYCY0S4bqUV_tVL6DELUjVcvLwCLcBGAs\/s1600\/poly2.jpg\" \/\u003E\u003C\/a\u003E\n\u003Cp\u003EHere we can create new features like \u003Ca href=\"https:\/\/2.bp.blogspot.com\/-cCV9hGqL9LQ\/Wln157jicDI\/AAAAAAAACMQ\/oiIreV5AsTYAB26KLHAI_fnoxbVMevuNgCLcBGAs\/s1600\/poly3.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\n\u003Cimg alt=\"polynomial new features\" data-original-height=\"29\" data-original-width=\"158\" height=\"35\" src=\"https:\/\/2.bp.blogspot.com\/-cCV9hGqL9LQ\/Wln157jicDI\/AAAAAAAACMQ\/oiIreV5AsTYAB26KLHAI_fnoxbVMevuNgCLcBGAs\/s200\/poly3.jpg\" width=\"200\" \/\u003E\u003C\/a\u003E\nand can fit linear regression in the similar manner.\u003C\/p\u003E\n\u003Cp\u003EIn case of multiple variables say X1 and X2, we can create a third new feature (say X3) which is the product of X1 and X2 i.e.\u003C\/p\u003E\n\u003Ca href=\"https:\/\/1.bp.blogspot.com\/-7PfDLmtSWJk\/Wln2md8NJ2I\/AAAAAAAACMc\/XpDcnrF4Md0jd-jmBXRI5yY_TgMnGWChACLcBGAs\/s1600\/poly5.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Cimg border=\"0\" data-original-height=\"27\" data-original-width=\"78\" src=\"https:\/\/1.bp.blogspot.com\/-7PfDLmtSWJk\/Wln2md8NJ2I\/AAAAAAAACMc\/XpDcnrF4Md0jd-jmBXRI5yY_TgMnGWChACLcBGAs\/s1600\/poly5.jpg\" \/\u003E\u003C\/a\u003E\n\u003Cp\u003E\u003Cb\u003EDisclaimer:\u003C\/b\u003E It is to be kept in mind that creating unnecessary extra features or fitting polynomials of higher degree may lead to overfitting.\n\u003C\/p\u003E\n\u003Cdiv class=\"db2\"\u003EPolynomial regression in R:\u003C\/div\u003E\n\u003Cp\u003E\u003Cspan style=\"color: #990000;\"\u003EWe are using \u003Ca href=\"https:\/\/github.com\/deepanshu88\/Datasets\/raw\/master\/UploadedFiles2\/poly.csv\" target=\"_blank\"\u003E\u003Cb\u003Epoly.csv \u003C\/b\u003E\u003C\/a\u003Edata for fitting polynomial regression where we try to estimate the Prices of the house given their area.\u003C\/span\u003E\n\u003C\/p\u003E\n\u003Cp\u003EFirstly we read the data using \u003Cb\u003Eread.csv( )\u003C\/b\u003Eand divide it into the dependent and independent variable\n\u003C\/p\u003E\u003Cpre\u003E\u003Ccode\u003E\ndata = read.csv(\"poly.csv\")\nx = data$Area\ny = data$Price\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EIn order to compare the results of linear and polynomial regression, firstly we fit linear regression:\n\u003C\/p\u003E\u003Cpre\u003E\u003Ccode\u003E\nmodel1 = lm(y ~x)\nmodel1$fit\nmodel1$coeff\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cdiv class='db2'\u003EOutput\u003C\/div\u003E   \n\u003Cp\u003EThe coefficients and predicted values obtained are:\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\n       1        2        3        4        5        6        7        8        9       10 \n169.0995 178.9081 188.7167 218.1424 223.0467 266.6949 291.7068 296.6111 316.2282 335.8454 \n\n(Intercept)            x \n120.05663769   0.09808581 \n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EWe create a dataframe where the new variable are x and x square.\n\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003Enew_x = cbind(x,x^2)\u003C\/code\u003E\u003C\/pre\u003E\nnew_x\n\u003Cpre\u003E\u003Ccode\u003E         x        \n [1,]  500  250000\n [2,]  600  360000\n [3,]  700  490000\n [4,] 1000 1000000\n [5,] 1050 1102500\n [6,] 1495 2235025\n [7,] 1750 3062500\n [8,] 1800 3240000\n [9,] 2000 4000000\n[10,] 2200 4840000\u003C\/code\u003E\u003C\/pre\u003E\nNow we fit usual OLS to the new data:\n\u003Cpre\u003E\u003Ccode\u003Emodel2 = lm(y~new_x)\nmodel2$fit\nmodel2$coeff\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EThe fitted values and regression coefficients of polynomial regression are:\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\u0026gt; model2$fit\n       1        2        3        4        5        6        7        8        9       10 \n122.5388 153.9997 182.6550 251.7872 260.8543 310.6514 314.1467 312.6928 299.8631 275.8110 \n\u0026gt; model2$coeff\n  (Intercept)        new_xx         new_x \n-7.684980e+01  4.689175e-01 -1.402805e-04 \n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EUsing ggplot2 package we try to create a plot to compare the curves by both linear and polynomial regression.\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\nlibrary(ggplot2)\nggplot(data = data) + geom_point(aes(x = Area,y = Price)) +\ngeom_line(aes(x = Area,y = model1$fit),color = \"red\") +\ngeom_line(aes(x = Area,y = model2$fit),color = \"blue\") +\ntheme(panel.background = element_blank())\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Ca href=\"https:\/\/3.bp.blogspot.com\/-GC6CZTGEsW0\/Wls-Q-ROh_I\/AAAAAAAACN0\/1USwBPjxa60fgR_0K62HH2XUVGIl8T7-wCLcBGAs\/s1600\/poly.jpeg\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Cimg border=\"0\" data-original-height=\"356\" data-original-width=\"500\" src=\"https:\/\/3.bp.blogspot.com\/-GC6CZTGEsW0\/Wls-Q-ROh_I\/AAAAAAAACN0\/1USwBPjxa60fgR_0K62HH2XUVGIl8T7-wCLcBGAs\/s1600\/poly.jpeg\" \/\u003E\u003C\/a\u003E\n\u003Ch3\u003E3. Logistic Regression\u003C\/h3\u003E\n\u003Cp\u003EIn logistic regression, the dependent variable is binary in nature (having two categories). Independent variables can be continuous or binary. In multinomial logistic regression, you can have more than two categories in your dependent variable.\n\u003C\/p\u003E\n\u003Cp\u003EHere my model is:\u003C\/p\u003E\n\u003Ctable align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Ca href=\"https:\/\/1.bp.blogspot.com\/-rsjL8rX6Q38\/VryuBND_wZI\/AAAAAAAAD_w\/Fpb9x6BfsuY\/s1600\/Optimized-prob%2Blogit%2B%25281%2529.png\" style=\"margin-left: auto; margin-right: auto;\"\u003E\u003Cimg alt=\"logistic regression\" border=\"0\" data-original-height=\"81\" data-original-width=\"314\" src=\"https:\/\/1.bp.blogspot.com\/-rsjL8rX6Q38\/VryuBND_wZI\/AAAAAAAAD_w\/Fpb9x6BfsuY\/s1600\/Optimized-prob%2Blogit%2B%25281%2529.png\" title=\"\" \/\u003E\u003C\/a\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003Ctr\u003E\u003Ctd class=\"tr-caption\" style=\"text-align: center;\"\u003Elogistic regression equation\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Cdiv class=\"db2\"\u003EWhy don't we use linear regression in this case?\u003C\/div\u003E\n\u003Cul\u003E\n\u003Cli\u003EHomoscedasticity assumption is violated.\u003C\/li\u003E\n\u003Cli\u003EErrors are not normally distributed\u003C\/li\u003E\n\u003Cli\u003Ey follows binomial distribution and hence is not normal.\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003Cdiv class=\"db2\"\u003EExamples\u003C\/div\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cb\u003EHR Analytics : \u003C\/b\u003EIT firms recruit large number of people, but one of the problems they encounter is after accepting the job offer many candidates do not join. So, this results in cost over-runs because they have to repeat the entire process again. Now when you get an application, can you actually predict whether that applicant is likely to join the organization (Binary Outcome - Join \/ Not Join).\u003C\/li\u003E\n\u003Cli\u003E\u003Cb\u003EElections : \u003C\/b\u003ESuppose that we are interested in the factors that influence whether a political candidate wins an election. The outcome (response) variable is binary (0\/1); win or lose. The predictor variables of interest are the amount of money spent on the campaign and the amount of time spent campaigning negatively.\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003Cp\u003EPredicting the category of dependent variable for a given vector X of independent variables.Through logistic regression we have - \n\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\nP(Y=1) = exp(a+ BₙX)\/ (1+ exp(a+ BₙX))\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EThus we choose a cut-off of probability say 'p' and if P(Yi = 1) \u0026gt; p then we can say that Yi belongs to class 1 otherwise 0.\u003C\/p\u003E\n\u003Cdiv class=\"db2\"\u003EInterpreting the logistic regression coefficients (Concept of Odds Ratio)\u003C\/div\u003E\n\u003Cp\u003EIf we take exponential of coefficients, then we’ll get odds ratio for ith explanatory variable. Suppose odds ratio is equal to two, then the odds of event is 2 times greater than the odds of non-event.  Suppose dependent variable is customer attrition (whether customer will close relationship with the company) and independent variable is citizenship status (National \/ Expat).  The odds of expat attrite is 3 times greater than the odds of a national attrite.\n\u003C\/p\u003E\n\u003Cdiv class=\"db2\"\u003ELogistic Regression in R:\u003C\/div\u003E\n\u003Cp\u003EIn this case, we are trying to estimate whether a person will have cancer depending whether he smokes or not.\u003C\/p\u003E\n\u003Cp\u003EWe fit logistic regression with \u003Cb\u003Eglm( )\u003C\/b\u003Efunction and we set\u003Cb\u003Efamily = \"binomial\"\u003C\/b\u003E\u003C\/p\u003E\n\u003Cpre\u003E\nmodel \u0026lt;- glm(Lung.Cancer..Y.~Smoking..X.,data = data, family = \"binomial\")\u003C\/pre\u003E\nThe predicted probabilities are given by:\n\u003Cpre\u003E\u003Ccode\u003E\n#Predicted Probablities\nmodel$fitted.values\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cpre\u003E\u003Ccode\u003E        1         2         3         4         5         6         7         8         9 \n0.4545455 0.4545455 0.6428571 0.6428571 0.4545455 0.4545455 0.4545455 0.4545455 0.6428571 \n       10        11        12        13        14        15        16        17        18 \n0.6428571 0.4545455 0.4545455 0.6428571 0.6428571 0.6428571 0.4545455 0.6428571 0.6428571 \n       19        20        21        22        23        24        25 \n0.6428571 0.4545455 0.6428571 0.6428571 0.4545455 0.6428571 0.6428571 \n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EPredicting whether the person will have cancer or not when we choose the cut off probability to be 0.5.\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\ndata$prediction \u0026lt;- model$fitted.values\u0026gt;0.5\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cpre\u003E\u003Ccode\u003E\u0026gt; data$prediction\n [1] FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n[16] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Ch3\u003E4. Quantile Regression\u003C\/h3\u003E\n\u003Cp\u003EQuantile regression is the extension of linear regression and we generally use it when outliers, high skeweness and heteroscedasticity exist in the data.\u003C\/p\u003E\n\u003Cp\u003EIn linear regression, we predict the mean of the dependent variable for given independent variables. Since mean does not describe the whole distribution, so modeling the mean is not a full description of a relationship between dependent and independent variables. So we can use quantile regression which predicts a quantile (or percentile) for given independent variables.\n\u003C\/p\u003E \u003Cp\u003EThe term \"quantile\" is the same as \"percentile\"\u003C\/p\u003E\n\u003Cdiv class=\"db2\"\u003EBasic Idea of Quantile Regression\u003C\/div\u003E\n\u003Cp\u003EIn quantile regression we try to estimate the quantile of the dependent variable given the values of X's. \u003Cb\u003ENote : \u003C\/b\u003E the dependent variable should be continuous.\u003C\/p\u003E\n\u003Cp\u003E\u003Cb\u003EQuantile regression model:\u003C\/b\u003E\u003C\/p\u003E\n\u003Cp\u003EFor qth quantile we have the following regression model:\u003C\/p\u003E\n\u003Cimg alt=\"Quantile Regression Model\" data-original-height=\"29\" data-original-width=\"96\" src=\"https:\/\/3.bp.blogspot.com\/-v3umgz7unTs\/Wllzkp4y5YI\/AAAAAAAACK4\/xAokU6rrQPMKyLsHtvn65bbIgfedSGMCwCLcBGAs\/s1600\/img5.jpg\" \/\u003E \n\u003Cp\u003EThis seems similar to linear regression model but here the objective function we consider to minimize is:\n\u003Ca href=\"https:\/\/1.bp.blogspot.com\/-iQrIMqXI4Rk\/Wll0V465mOI\/AAAAAAAACLA\/YPTqA4MhAGYE0u0P8NF23UTDIQM_R9PkQCLcBGAs\/s1600\/img6.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Cimg border=\"0\" data-original-height=\"48\" data-original-width=\"194\" src=\"https:\/\/1.bp.blogspot.com\/-iQrIMqXI4Rk\/Wll0V465mOI\/AAAAAAAACLA\/YPTqA4MhAGYE0u0P8NF23UTDIQM_R9PkQCLcBGAs\/s1600\/img6.jpg\" \/\u003E\u003C\/a\u003E\nwhere q is the qth quantile.\n\u003C\/p\u003E\n\u003Cp\u003EIf q = 0.5 i.e. if we are interested in the median then it becomes \u003Cb\u003Emedian regression \u003C\/b\u003E(or least absolute deviation regression) and substituting the value of q = 0.5 in above equation we get the objective function as:\n\u003C\/p\u003E\u003Cspan style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Ca href=\"https:\/\/2.bp.blogspot.com\/-W3cULkOl6vs\/Wll1AfbrzWI\/AAAAAAAACLM\/TYEZb0HImycDIAOMXh_dU1l7NeOWt9HVgCLcBGAs\/s1600\/img%2B7.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Cimg border=\"0\" data-original-height=\"59\" data-original-width=\"63\" src=\"https:\/\/2.bp.blogspot.com\/-W3cULkOl6vs\/Wll1AfbrzWI\/AAAAAAAACLM\/TYEZb0HImycDIAOMXh_dU1l7NeOWt9HVgCLcBGAs\/s1600\/img%2B7.jpg\" \/\u003E\u003C\/a\u003E\u003C\/span\u003E\n\u003Cdiv class=\"db2\"\u003EInterpreting the coefficients in quantile regression:\u003C\/div\u003E\n\u003Cp\u003ESuppose the regression equation for 25th quantile of regression is:\u003C\/p\u003E\n\u003Cp\u003Ey = 5.2333 + 700.823 x\n\u003C\/p\u003E\u003Cp\u003E\nIt means that for one unit increase in x the estimated increase in 25th quantile of y by 700.823 units.\n\u003C\/p\u003E\n\u003Cdiv class=\"db2\"\u003EAdvantages of Quantile over Linear Regression\u003C\/div\u003E\n\u003Cul\u003E\n\u003Cli\u003EQuite beneficial when heteroscedasticity is present in the data.\u003C\/li\u003E\n\u003Cli\u003ERobust to outliers\u003C\/li\u003E\n\u003Cli\u003EDistribution of dependent variable can be described via various quantiles.\u003C\/li\u003E\n\u003Cli\u003EIt is more useful than linear regression when the data is skewed.\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003Cdiv class=\"db2\"\u003EDisclaimer on using quantile regression!\u003C\/div\u003E\n\u003Cp\u003EIt is to be kept in mind that the coefficients which we get in quantile regression for a particular quantile should differ significantly from those we obtain from linear regression. If it is not so then our usage of quantile regression isn't justifiable. This can be done by observing the confidence intervals of regression coefficients of the estimates obtained from both the regressions.\u003C\/p\u003E\n\u003Cdiv class=\"db2\"\u003EQuantile Regression in R\u003C\/div\u003E\n\u003Cp\u003EWe need to install \u003Cb\u003Equantreg\u003C\/b\u003E package in order to carry out quantile regression.\n\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\ninstall.packages(\"quantreg\")\nlibrary(quantreg)\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EUsing \u003Cb\u003Erq \u003C\/b\u003Efunction we try to predict the estimate the 25th quantile of Fertility Rate in \u003Cb\u003ESwiss data. \u003C\/b\u003EFor this we set \u003Cb\u003Etau = 0.25.\u003C\/b\u003E\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\nmodel1 = rq(Fertility~.,data = swiss,tau = 0.25)\nsummary(model1)\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cpre\u003E\u003Ccode\u003Etau: [1] 0.25\nCoefficients:\n                 coefficients lower bd upper bd\n(Intercept)      76.63132      2.12518 93.99111\nAgriculture      -0.18242     -0.44407  0.10603\nExamination      -0.53411     -0.91580  0.63449\nEducation        -0.82689     -1.25865 -0.50734\nCatholic          0.06116      0.00420  0.22848\nInfant.Mortality  0.69341     -0.10562  2.36095\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003ESetting tau = 0.5 we run the median regression.\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\nmodel2 = rq(Fertility~.,data = swiss,tau = 0.5)\nsummary(model2)\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cpre\u003E\u003Ccode\u003Etau: [1] 0.5\nCoefficients:\n                 coefficients lower bd upper bd\n(Intercept)      63.49087     38.04597 87.66320\nAgriculture      -0.20222     -0.32091 -0.05780\nExamination      -0.45678     -1.04305  0.34613\nEducation        -0.79138     -1.25182 -0.06436\nCatholic          0.10385      0.01947  0.15534\nInfant.Mortality  1.45550      0.87146  2.21101\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EWe can run quantile regression for multiple quantiles in a single plot.\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\nmodel3 = rq(Fertility~.,data = swiss, tau = seq(0.05,0.95,by = 0.05))\nquantplot = summary(model3)\nquantplot\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EWe can check whether our quantile regression results differ from the OLS results using plots.\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\nplot(quantplot)\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EWe get the following plot:\n\u003C\/p\u003E\n\u003Ca href=\"https:\/\/3.bp.blogspot.com\/-hQ1Vi3BsoC8\/WlsZrq7z3mI\/AAAAAAAACNk\/PCxLz7EPpiIAtzUv3dWuNrluRr8mbo6dwCLcBGAs\/s1600\/quantplot.jpeg\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Cimg border=\"0\" data-original-height=\"356\" data-original-width=\"500\" height=\"452\" src=\"https:\/\/3.bp.blogspot.com\/-hQ1Vi3BsoC8\/WlsZrq7z3mI\/AAAAAAAACNk\/PCxLz7EPpiIAtzUv3dWuNrluRr8mbo6dwCLcBGAs\/s640\/quantplot.jpeg\" width=\"640\" \/\u003E\u003C\/a\u003E\n\u003Cp\u003EVarious quantiles are depicted by X axis. The red central line denotes the estimates of OLS coefficients and the dotted red lines are the confidence intervals around those OLS coefficients for various quantiles. The black dotted line are the \u003Cb\u003Equantile regression estimates \u003C\/b\u003Eand the gray area is the confidence interval for themfor various quantiles. We can see that for all the variable both the regression estimated coincide for most of the quantiles. Hence our use of quantile regression is not justifiable for such quantiles. In other words we want that both the red and the gray lines should overlap as less as possible to justify our use of quantile regression.\n\u003C\/p\u003E\n\u003Ch3\u003E5. Ridge Regression\u003C\/h3\u003E\n\u003Cp\u003EIt's important to understand the concept of regularization before jumping to ridge regression.\n\u003C\/p\u003E\u003Cdiv class=\"db\"\u003E1. Regularization\u003C\/div\u003E\n\u003Cp\u003ERegularization helps to solve over fitting problem which implies model performing well on training data but performing poorly on validation (test) data. Regularization solves this problem by adding a penalty term to the objective function and control the model complexity using that penalty term.\n\u003C\/p\u003E\n\u003Cp\u003ERegularization is generally useful in the following situations:\u003C\/p\u003E\n\u003Col\u003E\n\u003Cli\u003ELarge number of variables\u003C\/li\u003E\n\u003Cli\u003ELow ratio of number observations to number of variables\u003C\/li\u003E\n\u003Cli\u003EHigh Multi-Collinearity\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003Cdiv class=\"db\"\u003E2. L1 Loss function or L1 Regularization\u003C\/div\u003E\n\u003Cp\u003EIn L1 regularization we try to minimize the objective function by adding a penalty term to the \u003Cb\u003Esum of the absolute values of coefficients.\u003C\/b\u003E This is also known as least absolute deviations method. Lasso Regression makes use of L1 regularization.\n\u003C\/p\u003E\n\u003Cdiv class=\"db\"\u003E3. L2 Loss function or L2 Regularization\u003C\/div\u003E\n\u003Cp\u003EIn L2 regularization we try to minimize the objective function by adding a penalty term to the\u003Cb\u003Esum of the squares of coefficients.\u003C\/b\u003ERidgeRegression or shrinkage regression makes use of L2 regularization.\n\u003C\/p\u003E\n\u003Cp\u003EIn general, L2 performs better than L1 regularization. L2 is efficient in terms of computation. There is one area where L1 is considered as a preferred option over L2. L1 has in-built feature selection for sparse feature spaces. For example, you are predicting whether a person is having a brain tumor using more than 20,000 genetic markers (features). It is known that the vast majority of genes have little or no effect on the presence or severity of most diseases.\u003C\/p\u003E\n\u003Cp\u003EIn the linear regression objective function we try to minimize the sum of squares of errors. \u003Cb\u003EIn ridge regression\u003C\/b\u003E (also known as shrinkage regression) we add a constraint on the sum of squares of the regression coefficients. Thus in ridge regression our objective function is:\n\u003C\/p\u003E\n\u003Ca href=\"https:\/\/2.bp.blogspot.com\/-mQhz_RBk_Rs\/WlrsUXF0U3I\/AAAAAAAACMs\/OZ2nOGaYYVk457X9Y3h1cC0d_ajcMTUDACLcBGAs\/s1600\/ridge1.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Cimg border=\"0\" data-original-height=\"38\" data-original-width=\"461\" src=\"https:\/\/2.bp.blogspot.com\/-mQhz_RBk_Rs\/WlrsUXF0U3I\/AAAAAAAACMs\/OZ2nOGaYYVk457X9Y3h1cC0d_ajcMTUDACLcBGAs\/s1600\/ridge1.jpg\" \/\u003E\u003C\/a\u003E\n\u003Cp\u003EHere \u003Cb\u003Eλ is the regularization parameter\u003C\/b\u003E which is a non negative number. Here we do not assume normality in the error terms.\n\u003C\/p\u003E\n\u003Cdiv class=\"db2\"\u003EVery Important Note:\u003C\/div\u003E\n\u003Cp\u003EWe do not regularize the intercept term. The constraint is just on the sum of squares of regression coefficients of X's.\u003C\/p\u003E\n\u003Cp\u003EWe can see that ridge regression makes use of \u003Cb\u003EL2 regularization.\u003C\/b\u003E\n\u003C\/p\u003E\n\u003Cp\u003EOn solving the above objective function we can get the estimates of β as:\u003C\/p\u003E\n\u003Cimg border=\"0\" data-original-height=\"64\" data-original-width=\"314\" src=\"https:\/\/3.bp.blogspot.com\/-CDhsY4B8YWQ\/WlrunCkg0KI\/AAAAAAAACM4\/ff1hN83GwjIjSdA1tBOhVxF_TI6xwzXpQCLcBGAs\/s1600\/ridge2.jpg\" \/\u003E\n\u003Cdiv class=\"db2\"\u003EHow can we choose the regularization parameter λ?\u003C\/div\u003E\n\u003Cp\u003EIf we choose lambda = 0 then we get back to the usual OLS estimates. If lambda is chosen to be very large then it will lead to underfitting. Thus it is highly important to determine a desirable value of lambda.To tackle this issue, we plot the parameter estimates against different values of lambda and select the minimum value of λ after which the parameters tend to stabilize.\n\u003C\/p\u003E\n\u003Cdiv class=\"db\"\u003ER code for Ridge Regression\u003C\/div\u003E\n\u003Cp\u003EConsidering the swiss data set, we create two different datasets, one containing dependent variable and other containing independent variables.\n\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\nX = swiss[,-1]\ny = swiss[,1]\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EWe need to load \u003Cb\u003Eglmnet\u003C\/b\u003E library to carry out ridge regression.\n\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\nlibrary(glmnet)\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EUsing \u003Cb\u003Ecv.glmnet( )\u003C\/b\u003E function we can do cross validation. By default\u003Cb\u003E alpha = 0\u003C\/b\u003E which means we are carrying out ridge regression. \u003Cb\u003Elambda\u003C\/b\u003E is a sequence of various values of lambda which will be used for cross validation.\n\u003C\/p\u003E\u003Cpre\u003E\u003Ccode\u003E\nset.seed(123) #Setting the seed to get similar results.\nmodel = cv.glmnet(as.matrix(X),y,alpha = 0,lambda = 10^seq(4,-1,-0.1))\n\u003C\/code\u003E\u003C\/pre\u003E\nWe take the best lambda by using \u003Cb\u003Elambda.min\u003C\/b\u003E and hence get the regression coefficients using \u003Cb\u003Epredict \u003C\/b\u003Efunction.\n\u003Cpre\u003E\u003Ccode\u003E\nbest_lambda = model$lambda.min\nridge_coeff = predict(model,s = best_lambda,type = \"coefficients\")\nridge_coeff\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EThe coefficients obtained using ridge regression are:\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E6 x 1 sparse Matrix of class \"dgCMatrix\"\n                           1\n(Intercept)      64.92994664\nAgriculture      -0.13619967\nExamination      -0.31024840\nEducation        -0.75679979\nCatholic          0.08978917\nInfant.Mortality  1.09527837\u003C\/code\u003E\u003C\/pre\u003E\n\u003Ch3\u003E6. Lasso Regression\u003C\/h3\u003E\n\u003Cp\u003ELasso stands for \u003Cb\u003ELeast Absolute Shrinkage and Selection Operator\u003C\/b\u003E. It makes use of\u003Cb\u003E L1 regularization \u003C\/b\u003E technique in the objective function. Thus the objective function in LASSO regression becomes:\n\u003C\/p\u003E\u003Ca href=\"https:\/\/1.bp.blogspot.com\/-vy7hx5HBZog\/WlrxD3WTeGI\/AAAAAAAACNE\/qXVszdrqEmEhd8FLFT_Hz6uu3MXVzVXBwCLcBGAs\/s1600\/lasso%2B1.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\n\u003Cimg alt=\"Lasso Regression\" data-original-height=\"42\" data-original-width=\"472\" src=\"https:\/\/1.bp.blogspot.com\/-vy7hx5HBZog\/WlrxD3WTeGI\/AAAAAAAACNE\/qXVszdrqEmEhd8FLFT_Hz6uu3MXVzVXBwCLcBGAs\/s1600\/lasso%2B1.jpg\" \/\u003E\u003C\/a\u003E\n\u003Cp\u003Eλ is the regularization parameter and the intercept term is not regularized.\u003C\/p\u003E\n\u003Cp\u003EWe do not assume that the error terms are normally distributed.\n\u003C\/p\u003E\u003Cp\u003EFor the estimates we don't have any specific mathematical formula but we can obtain the estimates using some statistical software.\u003C\/p\u003E\n\u003Cp\u003E\u003Cb\u003E\u003Ci\u003ENote that lasso regression also needs standardization.\u003C\/i\u003E\u003C\/b\u003E\u003C\/p\u003E\n\u003Cdiv class=\"db\"\u003EAdvantage of lasso over ridge regression\u003C\/div\u003E\n\u003Cp\u003ELasso regression can perform in-built variable selection as well as parameter shrinkage. While using ridge regression one may end up getting all the variables but with \u003Cb\u003EShrinked Paramaters.\u003C\/b\u003E\n\u003C\/p\u003E\n\u003Cdiv class=\"db\"\u003ER code for Lasso Regression\u003C\/div\u003E\n\u003Cp\u003EConsidering the \u003Cb\u003Eswiss dataset\u003C\/b\u003E from \"\u003Cb\u003Edatasets\u003C\/b\u003E\" package, we have:\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\n#Creating dependent and independent variables.\nX = swiss[,-1]\ny = swiss[,1]\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EUsing\u003Cb\u003E cv.glmnet \u003C\/b\u003Ein \u003Cb\u003Eglmnet \u003C\/b\u003Epackage we do cross validation. For lasso regression we set alpha = 1. By default standardize = TRUE hence we do not need to standardize the variables seperately.\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\n#Setting the seed for reproducibility\nset.seed(123)\nmodel = cv.glmnet(as.matrix(X),y,alpha = 1,lambda = 10^seq(4,-1,-0.1))\n#By default standardize = TRUE\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EWe consider the best value of lambda by filtering out \u003Cb\u003Elamba.min\u003C\/b\u003E from the model and hence get the coefficients using \u003Cb\u003Epredict \u003C\/b\u003Efunction.\n\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\n#Taking the best lambda\nbest_lambda = model$lambda.min\nlasso_coeff = predict(model,s = best_lambda,type = \"coefficients\")\nlasso_coeff\nThe lasso coefficients we got are:\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cpre\u003E\u003Ccode\u003E6 x 1 sparse Matrix of class \"dgCMatrix\"\n                           1\n(Intercept)      65.46374579\nAgriculture      -0.14994107\nExamination      -0.24310141\nEducation        -0.83632674\nCatholic          0.09913931\nInfant.Mortality  1.07238898\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cdiv class=\"db2\"\u003EWhich one is better - Ridge regression or Lasso regression?\u003C\/div\u003E\n\u003Cp\u003EBoth ridge regression and lasso regression are addressed to deal with multicollinearity. Ridge regression is computationally more efficient over lasso regression.Any of them can perform better. So the best approach is to \u003Cb\u003Eselect that regression model which fits the test set data well.\u003C\/b\u003E\u003C\/p\u003E\n\u003Ch3\u003E7. Elastic Net Regression\u003C\/h3\u003E\n\u003Cp\u003EElastic Net regression is preferred over both ridge and lasso regression when one is dealing with highly correlated independent variables.\u003C\/p\u003E\n\u003Cp\u003EIt is a \u003Ccode\u003Ecombination of both L1 and L2 regularization\u003C\/code\u003E.\u003C\/p\u003E\n\u003Cp\u003EThe objective function in case of Elastic Net Regression is:\u003C\/p\u003E\n\u003Ca href=\"https:\/\/1.bp.blogspot.com\/-1C5RJPcZ1vk\/Wlr2qDJsijI\/AAAAAAAACNU\/3bSXMjEcM5ALaeTsliyhvysR-ASuCFX4QCLcBGAs\/s1600\/elasticnet1.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\n\u003Cimg alt=\"Elastic Net Regression\" data-original-height=\"50\" data-original-width=\"606\" src=\"https:\/\/1.bp.blogspot.com\/-1C5RJPcZ1vk\/Wlr2qDJsijI\/AAAAAAAACNU\/3bSXMjEcM5ALaeTsliyhvysR-ASuCFX4QCLcBGAs\/s1600\/elasticnet1.jpg\" \/\u003E\u003C\/a\u003E\n\u003Cp\u003ELike ridge and lasso regression, it does not assume normality.\u003C\/p\u003E\n\u003Cdiv class=\"db\"\u003ER code for Elastic Net Regression\u003C\/div\u003E\n\u003Cp\u003ESetting some different value of alpha between 0 and 1 we can carry out elastic net regression.\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\nset.seed(123)\nmodel = cv.glmnet(as.matrix(X),y,alpha = 0.5,lambda = 10^seq(4,-1,-0.1))\n#Taking the best lambda\nbest_lambda = model$lambda.min\nen_coeff = predict(model,s = best_lambda,type = \"coefficients\")\nen_coeff\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EThe coeffients we obtained are:\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E6 x 1 sparse Matrix of class \"dgCMatrix\"\n                          1\n(Intercept)      65.9826227\nAgriculture      -0.1570948\nExamination      -0.2581747\nEducation        -0.8400929\nCatholic          0.0998702\nInfant.Mortality  1.0775714\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Ch3\u003E8. Principal Components Regression (PCR)\u003C\/h3\u003E\n\u003Cp\u003EPCR is a regression technique which is widely used when you have many independent variables OR multicollinearity exist in your data. It is divided into 2 steps:\n\u003C\/p\u003E\u003Col\u003E\n\u003Cli\u003EGetting the Principal components\u003C\/li\u003E\n\u003Cli\u003ERun regression analysis on principal components\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003Cp\u003EThe most common features of PCR are:\u003C\/p\u003E\n\u003Col\u003E\n\u003Cli\u003EDimensionality Reduction\u003C\/li\u003E\n\u003Cli\u003ERemoval of multicollinearity\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003Cdiv class='db2'\u003EGetting the Principal components\u003C\/div\u003E\n\u003Cp\u003EPrincipal components analysis is a statistical method to extract new features when the original features are highly correlated. We create new features with the help of original features such that the new features are uncorrelated.\n\u003C\/p\u003E\n\u003Cspan style=\"color: #990000;\"\u003ELet us consider the first principal component:\u003C\/span\u003E\n\u003Ca href=\"https:\/\/3.bp.blogspot.com\/-vFggXtE8i3k\/WltkDFPjqFI\/AAAAAAAACOw\/ve6r9aZqK_QJ8ZXLcKkORF3__QMLnJuSgCLcBGAs\/s1600\/pca1.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\n\u003Cimg alt=\"First Principal Component\" data-original-height=\"66\" data-original-width=\"176\" src=\"https:\/\/3.bp.blogspot.com\/-vFggXtE8i3k\/WltkDFPjqFI\/AAAAAAAACOw\/ve6r9aZqK_QJ8ZXLcKkORF3__QMLnJuSgCLcBGAs\/s1600\/pca1.jpg\" \/\u003E\u003C\/a\u003E\n\u003Cp\u003EThe first PC is having the maximum variance. Similarly we can find the second PC U2 such that it is \u003Cb\u003Euncorrelated\u003C\/b\u003E with U1 and has the second largest variance.\n\u003C\/p\u003E\u003Cp\u003EIn a similar manner for 'p' features we can have a maximum of 'p' PCs such that all the PCs are uncorrelated with each other and the first PC has the maximum variance, then 2nd PC has the maximum variance and so on.\n\u003C\/p\u003E\u003Cdiv class='db2'\u003EDrawbacks:\u003C\/div\u003E\n\u003Cp\u003EIt is to be mentioned that PCR is not a feature selection technique instead it is a feature extraction technique. Each principle component we obtain is a function of all the features. Hence on using principal components one would be unable to explain which factor is affecting the dependent variable to what extent.\n\u003C\/p\u003E\n\u003Cdiv class=\"db2\"\u003EPrincipal Components Regression in R\u003C\/div\u003E\n\u003Cp\u003EWe use the longley data set available in R which is used for high multicollinearity. We excplude the Year column.\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\ndata1 = longley[,colnames(longley) != \"Year\"]\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EThis is how some of the observations in our dataset will look like:\u003C\/p\u003E\n\u003Ca href=\"https:\/\/4.bp.blogspot.com\/-B6eJCR7lOOQ\/WltvJ8-LTaI\/AAAAAAAACPA\/8myMa8mnSPcJLolt2U5u_zNTnYvdph8fgCLcBGAs\/s1600\/pca2.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Cimg border=\"0\" data-original-height=\"368\" data-original-width=\"479\" src=\"https:\/\/4.bp.blogspot.com\/-B6eJCR7lOOQ\/WltvJ8-LTaI\/AAAAAAAACPA\/8myMa8mnSPcJLolt2U5u_zNTnYvdph8fgCLcBGAs\/s1600\/pca2.jpg\" \/\u003E\u003C\/a\u003E\n\u003Cp\u003EWe use \u003Cb\u003Epls package\u003C\/b\u003Ein order to run PCR.\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\ninstall.packages(\"pls\")\nlibrary(pls)\n\u003C\/code\u003E\u003C\/pre\u003E\nIn PCR we are trying to estimate the number of Employed people; scale = T denotes that we are standardizing the variables; validation = \"CV\" denotes applicability of cross-validation.\n\u003Cpre\u003E\u003Ccode\u003E\npcr_model \u0026lt;- pcr(Employed~., data = data1, scale = TRUE, validation = \"CV\")\nsummary(pcr_model)\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EWe get the summary as:\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003EData:  X dimension: 16 5 \n Y dimension: 16 1\nFit method: svdpc\nNumber of components considered: 5\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps\nCV           3.627    1.194    1.118   0.5555   0.6514   0.5954\nadjCV        3.627    1.186    1.111   0.5489   0.6381   0.5819\nTRAINING: % variance explained\n          1 comps  2 comps  3 comps  4 comps  5 comps\nX           72.19    95.70    99.68    99.98   100.00\nEmployed    90.42    91.89    98.32    98.33    98.74\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EHere in the RMSEP the root mean square errors are being denoted. While in 'Training: %variance explained' the cumulative % of variance explained by principle components is being depicted. We can see that with 3 PCs more than 99% of variation can be attributed.\u003C\/p\u003E\n\u003Cp\u003EWe can also create a plot depicting the mean squares error for the number of various PCs.\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\nvalidationplot(pcr_model,val.type = \"MSEP\")\u003C\/code\u003E\u003C\/pre\u003E\n\u003Ca href=\"https:\/\/3.bp.blogspot.com\/-_v_Uv2PL1UQ\/WltwxC64CqI\/AAAAAAAACPM\/qFUihMP8RPM590m466Dm-DHiyRSPEW7RgCLcBGAs\/s1600\/pca3.jpeg\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Cimg border=\"0\" data-original-height=\"356\" data-original-width=\"500\" src=\"https:\/\/3.bp.blogspot.com\/-_v_Uv2PL1UQ\/WltwxC64CqI\/AAAAAAAACPM\/qFUihMP8RPM590m466Dm-DHiyRSPEW7RgCLcBGAs\/s1600\/pca3.jpeg\" \/\u003E\u003C\/a\u003E\n\u003Cp\u003EBy writing \u003Cb\u003Eval.type = \"R2\" \u003C\/b\u003Ewe can plot the R square for various no. of PCs.\n\u003C\/p\u003E\u003Cpre\u003E\u003Ccode\u003E\nvalidationplot(pcr_model,val.type = \"R2\")\u003C\/code\u003E\u003C\/pre\u003E\n\u003Ca href=\"https:\/\/4.bp.blogspot.com\/-sHy2Oag4RSM\/WltxEPdbeSI\/AAAAAAAACPQ\/hFAfZSVAFsEK_C8ZTNRaxTUIv0lrmsbsQCLcBGAs\/s1600\/pca4.jpeg\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Cimg border=\"0\" data-original-height=\"356\" data-original-width=\"500\" src=\"https:\/\/4.bp.blogspot.com\/-sHy2Oag4RSM\/WltxEPdbeSI\/AAAAAAAACPQ\/hFAfZSVAFsEK_C8ZTNRaxTUIv0lrmsbsQCLcBGAs\/s1600\/pca4.jpeg\" \/\u003E\u003C\/a\u003E\n\u003Cp\u003EIf we want to fit pcr for 3 principal components and hence get the predicted values we can write:\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\npred = predict(pcr_model,data1,ncomp = 3)\u003C\/code\u003E\u003C\/pre\u003E\n\u003Ch3\u003E9. Partial Least Squares (PLS) Regression\u003C\/h3\u003E\n\u003Cp\u003EIt is an alternative technique of principal component regression when you have independent variables highly correlated. It is also useful when there are a large number of independent variables.\n\u003C\/p\u003E\n\u003Cdiv class=\"db2\"\u003EDifference between PLS and PCR\u003C\/div\u003E\n\u003Cp\u003EBoth techniques create new independent variables called components which are linear combinations of the original predictor variables but PCR creates components to explain the observed variability in the predictor variables, without considering the response variable at all. While PLS takes the dependent variable into account, and therefore often leads to models that are able to fit the dependent variable with fewer components.\u003C\/p\u003E\n\u003Cdiv class=\"db2\"\u003EPLS Regression in R\u003C\/div\u003E\n\u003Cpre\u003E\u003Ccode\u003E\nlibrary(plsdepot)\ndata(vehicles)\npls.model = plsreg1(vehicles[, c(1:12,14:16)], vehicles[, 13], comps = 3)\n# R-Square\npls.model$R2\u003C\/code\u003E\u003C\/pre\u003E\n\u003Ch3\u003E10. Support Vector Regression\u003C\/h3\u003E\n\u003Cp\u003ESupport vector regression can solve both linear and non-linear models. SVM uses non-linear kernel functions (such as polynomial) to find the optimal solution for non-linear models.\n\u003C\/p\u003E\u003Cp\u003EThe main idea of SVR is to minimize error, individualizing the hyperplane which maximizes the margin.\n\u003C\/p\u003E\u003Cpre\u003E\u003Ccode\u003E\nlibrary(e1071)\nsvr.model \u0026lt;- svm(Y ~ X , data)\npred \u0026lt;- predict(svr.model, data)\npoints(data$X, pred, col = \"red\", pch=4)\u003C\/code\u003E\u003C\/pre\u003E\n\u003Ch3\u003E11. Ordinal Regression\u003C\/h3\u003E\n\u003Cp\u003EOrdinal Regression is used to \u003Cb\u003Epredict ranked values\u003C\/b\u003E. In simple words, this type of regression is suitable when dependent variable is ordinal in nature. \u003Cb\u003EExample of ordinal variables -\u003C\/b\u003E Survey responses (1 to 6 scale), patient reaction to drug dose (none, mild, severe).\u003C\/p\u003E\n\u003Cdiv class=\"db2\"\u003EWhy we can't use linear regression when dealing with ordinal target variable?\u003C\/div\u003E\n\u003Cp\u003EIn linear regression, the dependent variable assumes that changes in the level of the dependent variable are equivalent throughout the range of the variable. For example, the difference in weight between a person who is 100 kg and a person who is 120 kg is 20kg, which has the same meaning as the difference in weight between a person who is 150 kg and a person who is 170 kg. These relationships do not necessarily hold for ordinal variables.\n\u003C\/p\u003E\u003Cpre\u003E\u003Ccode\u003E\nlibrary(ordinal)\no.model \u0026lt;- clm(rating ~ ., data = wine)\nsummary(o.model)\u003C\/code\u003E\u003C\/pre\u003E\n\u003Ch3\u003E12. Poisson Regression\u003C\/h3\u003E\n\u003Cp\u003EPoisson regression is used \u003Cb\u003Ewhen dependent variable has count data\u003C\/b\u003E.\u003C\/p\u003E\n\u003Cdiv class=\"db2\"\u003EApplication of Poisson Regression -\u003C\/div\u003E\n\u003Col\u003E\n\u003Cli\u003EPredicting the number of calls in customer care related to a particular product\u003C\/li\u003E\n\u003Cli\u003EEstimating the number of emergency service calls during an event\u003C\/li\u003E\n\u003C\/ol\u003E\nThe dependent variable must meet the following conditions -\n\u003Col\u003E\n\u003Cli\u003EThe dependent variable has a Poisson distribution.\u003C\/li\u003E\n\u003Cli\u003ECounts cannot be negative.\u003C\/li\u003E\n\u003Cli\u003EThis method is not suitable on non-whole numbers\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003Cp\u003EIn the code below, we are using dataset named warpbreaks which shows the number of breaks in Yarn during weaving. In this case, the model includes terms for wool type, wool tension and the interaction between the two.\n\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\npos.model\u0026lt;-glm(breaks~wool*tension, data = warpbreaks, family=poisson)\nsummary(pos.model)\u003C\/code\u003E\u003C\/pre\u003E\n\u003Ch3\u003E13. Negative Binomial Regression\u003C\/h3\u003E\n\u003Cp\u003ELike Poisson Regression, it also deals with count data. The question arises \"how it is different from poisson regression\". The answer is negative binomial regression does not assume distribution of count having variance equal to its mean. While poisson regression assumes the variance equal to its mean.\n\u003C\/p\u003E\u003Cp\u003EWhen the variance of count data is greater than the mean count, it is a case of \u003Cb\u003Eoverdispersion\u003C\/b\u003E. The opposite of the previous statement is a case of under-dispersion.\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\nlibrary(MASS)\nnb.model \u0026lt;- glm.nb(Days ~ Sex\/(Age + Eth*Lrn), data = quine)\nsummary(nb.model)\u003C\/code\u003E\u003C\/pre\u003E\n\u003Ch3\u003E14. Quasi Poisson Regression\u003C\/h3\u003E\n\u003Cp\u003EIt is an alternative to negative binomial regression. \u003Cb\u003EIt can also be used for overdispersed count data.\u003C\/b\u003E Both the algorithms give similar results, there are differences in estimating the effects of covariates. The variance of a quasi-Poisson model is a linear function of the mean while the variance of a negative binomial model is a quadratic function of the mean.\n\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\nqs.pos.model \u0026lt;- glm(Days ~ Sex\/(Age + Eth*Lrn), data = quine, family = \"quasipoisson\")\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EQuasi-Poisson regression can handle both over-dispersion and under-dispersion.\n\u003C\/p\u003E\n\u003Ch3\u003E15. Cox Regression\u003C\/h3\u003E\n\u003Cp\u003ECox Regression is suitable for time-to-event data. See the examples below -\u003C\/p\u003E\n\u003Col\u003E\n\u003Cli\u003ETime from customer opened the account until attrition.\u003C\/li\u003E\n\u003Cli\u003ETime after cancer treatment until death.\u003C\/li\u003E\n\u003Cli\u003ETime from first heart attack to the second.\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003Cp\u003ELogistic regression uses a binary dependent variable but ignores the timing of events.\u003C\/p\u003E\n\u003Cp\u003EAs well as estimating the time it takes to reach a certain event, survival analysis can also be used to compare time-to-event for multiple groups.\n\u003C\/p\u003E\n\u003Cp\u003EDual targets are set for the survival model\u003C\/p\u003E\n\u003Cp\u003E1. A continuous variable representing the time to event.\u003C\/p\u003E\n\u003Cp\u003E2. A binary variable representing the status whether event occurred or not.\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\nlibrary(survival)\n# Lung Cancer Data\n# status:\u003Cspan style=\"white-space: pre;\"\u003E \u003C\/span\u003E2=death\nlung$SurvObj \u0026lt;- with(lung, Surv(time, status == 2))\ncox.reg \u0026lt;- coxph(SurvObj ~ age + sex + ph.karno + wt.loss, data = lung)\ncox.reg\u003C\/code\u003E\u003C\/pre\u003E\n\u003Ch3\u003E16. Tobit Regression\u003C\/h3\u003E\n\u003Cp\u003EIt is used to estimate linear relationships between variables when censoring exists in the dependent variable.Censoring means when we observe independent variable for all observations, but we only know the true value of dependent variable for a restricted range of observations. Values\nof dependent in a certain range are reported as a single value. Detailed explanation of censoring is given below -\n\u003C\/p\u003E\n\u003Col\u003E\n\u003Cli\u003ERight censoring occurs when the event of interest does not occur before the end of study.  The study can't wait for an event from a subject before the considered study period ends. Suppose you are building a customer attrition model in which dependent variable is binary (Attrited or still with the company).For those customers who are still with the company (not attrited) even when study ends (2 years) are right-censored.\u003C\/li\u003E\n\u003Cli\u003ELeft censoring is when the event of interest has already occurred before enrollment. This is very rarely encountered\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003Cdiv class=\"db\"\u003ELimitation of the Tobit model\u003C\/div\u003E\n\u003Col\u003E\n\u003Cli\u003EThe Tobit model makes the same assumptions about error distributions as the OLS model, but it is much more vulnerable to violations of those assumptions.\u003C\/li\u003E\n\u003Cli\u003EIn an OLS model with heteroskedastic errors, the estimated standard errors can be too small\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003Ch2 class='top2'\u003EHow to choose the correct regression model?\u003C\/h2\u003E\n\u003Col\u003E\n\u003Cli\u003EIf dependent variable is continuous and model is suffering from collinearity or there are a lot of independent variables, you can try PCR, PLS, ridge, lasso and elastic net regressions. You can select the final model based on Adjusted r-square, RMSE, AIC and BIC.\u003C\/li\u003E\n\u003Cli\u003EIf you are working on count data, you should try poisson, quasi-poisson and negative binomial regression.\u003C\/li\u003E\n\u003Cli\u003ETo avoid overfitting, we can use cross-validation method to evaluate models used for prediction. We can also use ridge, lasso and elastic net regressions techniques to correct overfitting issue.\u003C\/li\u003E\n\u003Cli\u003ETry support vector regression when you have non-linear model.\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003C\/div\u003E\u003Cdiv class=\"blogger-post-footer\"\u003EThis post appeared first on \u003Ca href='https:\/\/www.listendata.com\/'\u003EListenData\u003C\/a\u003E\u003C\/div\u003E"},"link":[{"rel":"replies","type":"application/atom+xml","href":"https:\/\/www.listendata.com\/feeds\/5116849402452216087\/comments\/default","title":"Post Comments"},{"rel":"replies","type":"text/html","href":"https:\/\/www.listendata.com\/2018\/03\/regression-analysis.html#comment-form","title":"31 Comments"},{"rel":"edit","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/5116849402452216087"},{"rel":"self","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/5116849402452216087"},{"rel":"alternate","type":"text/html","href":"https:\/\/www.listendata.com\/2018\/03\/regression-analysis.html","title":"15 Types of Regression (with Examples)"}],"author":[{"name":{"$t":"Unknown"},"email":{"$t":"noreply@blogger.com"},"gd$image":{"rel":"http://schemas.google.com/g/2005#thumbnail","width":"16","height":"16","src":"https:\/\/img1.blogblog.com\/img\/b16-rounded.gif"}}],"media$thumbnail":{"xmlns$media":"http://search.yahoo.com/mrss/","url":"https:\/\/4.bp.blogspot.com\/-fBXCbdE9PLg\/XP5nZmMTzfI\/AAAAAAAAHnU\/w3c-fAwT_u81HGQv0_bf794mwaYMK7RYQCLcBGAs\/s72-c\/regression_R.PNG","height":"72","width":"72"},"thr$total":{"$t":"31"}},{"id":{"$t":"tag:blogger.com,1999:blog-7958828565254404797.post-844092150158921523"},"published":{"$t":"2018-01-10T02:08:00.002-08:00"},"updated":{"$t":"2022-05-19T08:26:11.964-07:00"},"category":[{"scheme":"http://www.blogger.com/atom/ns#","term":"Data Science"},{"scheme":"http://www.blogger.com/atom/ns#","term":"Linear Regression"},{"scheme":"http://www.blogger.com/atom/ns#","term":"Python"}],"title":{"type":"text","$t":"A Complete Guide to Linear Regression in Python"},"content":{"type":"html","$t":"In this article we covered linear regression using Python in detail. It includes its meaning along with assumptions related to the linear regression technique. After completing this tutorial you will be able to test these assumptions as well as model development and validation in Python.\n\u003Cbr\u003E\u003Cdiv id=\"toc\"\u003E\n\u003Cspan\u003ETable of Contents\u003C\/span\u003E\u003C\/div\u003E\n\u003Chr\u003E\n\u003Cdiv id=\"contents\"\u003E\n\u003Ctable align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"\u003E\u003Ctbody\u003E\n\u003Ctr\u003E\u003Ctd style=\"text-align: center;\"\u003E\u003Ca href=\"https:\/\/4.bp.blogspot.com\/-GwuWYL9RWN4\/WlXkOS72oJI\/AAAAAAAAGpU\/2TGJdC3Ni6kgsFAoyAiVaU31NghbncEUgCLcBGAs\/s1600\/Linear%2Bregression.PNG\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"\u003E\u003Cimg border=\"0\" data-original-height=\"224\" data-original-width=\"458\" src=\"https:\/\/4.bp.blogspot.com\/-GwuWYL9RWN4\/WlXkOS72oJI\/AAAAAAAAGpU\/2TGJdC3Ni6kgsFAoyAiVaU31NghbncEUgCLcBGAs\/s1600\/Linear%2Bregression.PNG\"\u003E\u003C\/a\u003E\u003C\/td\u003E\u003C\/tr\u003E\n\u003Ctr\u003E\u003Ctd class=\"tr-caption\" style=\"text-align: center;\"\u003EPython : Linear Regression\u003C\/td\u003E\u003C\/tr\u003E\n\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Ch2 style=\"margin-top: 1em;\"\u003EIntroduction to Linear Regression\u003C\/h2\u003E\nLinear Regression is a supervised statistical technique where we try to estimate the dependent variable with a given set of independent variables. We assume the relationship to be linear and our dependent variable must be continuous in nature.\u003Cbr\u003E\nIn the following diagram we can see that as horsepower increases mileage decreases thus we can think to fit linear regression. The red line is the fitted line of regression and the points denote the actual observations.\u003Cbr\u003E\n\u003Ca href=\"https:\/\/3.bp.blogspot.com\/-9Aeg6sCftPQ\/Wkzczq3TkEI\/AAAAAAAACFQ\/2POumT3SeS0phWdv4iSZNRf2wCEffTCJgCLcBGAs\/s1600\/a.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Cimg border=\"0\" data-original-height=\"279\" data-original-width=\"470\" src=\"https:\/\/3.bp.blogspot.com\/-9Aeg6sCftPQ\/Wkzczq3TkEI\/AAAAAAAACFQ\/2POumT3SeS0phWdv4iSZNRf2wCEffTCJgCLcBGAs\/s1600\/a.jpg\"\u003E\u003C\/a\u003E\n\u003Cbr\u003E\nThe vertical distance between the points and the fitted line (line of best fit) are called errors. The main idea is to fit this line of regression by minimizing the sum of squares of these errors. This is also known as \u003Cb\u003Eprinciple of least squares.\u003C\/b\u003E\u003Cbr\u003E\n\u003Cbr\u003E\n\u003Ch2 style=\"margin-top: 1em;\"\u003EExamples of Linear Regression\u003C\/h2\u003E\n\u003Cul style=\"text-align: left;\"\u003E\n\u003Cli\u003EEstimating the price (Y) of a house on the basis of its Area (X1), Number of bedrooms (X2), proximity to market (X3) etc. \u003C\/li\u003E\n\u003Cli\u003EEstimating the mileage of a car (Y) on the basis of its displacement (X1), horsepower(X2), number of cylinders(X3), whether it is automatic or manual (X4) etc. \u003C\/li\u003E\n\u003Cli\u003ETo find the treatment cost or to predict the treatment cost on the basis of factors like age, weight, past medical history, or even if there are blood reports, we can use the information from the blood report.\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003C\/div\u003E\u003Ca href=\"https:\/\/www.listendata.com\/2018\/01\/linear-regression-in-python.html#more\"\u003ETo read this article in full, please click here\u003C\/a\u003E\u003Cdiv class=\"blogger-post-footer\"\u003EThis post appeared first on \u003Ca href='https:\/\/www.listendata.com\/'\u003EListenData\u003C\/a\u003E\u003C\/div\u003E"},"link":[{"rel":"replies","type":"application/atom+xml","href":"https:\/\/www.listendata.com\/feeds\/844092150158921523\/comments\/default","title":"Post Comments"},{"rel":"replies","type":"text/html","href":"https:\/\/www.listendata.com\/2018\/01\/linear-regression-in-python.html#comment-form","title":"10 Comments"},{"rel":"edit","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/844092150158921523"},{"rel":"self","type":"application/atom+xml","href":"https:\/\/www.blogger.com\/feeds\/7958828565254404797\/posts\/default\/844092150158921523"},{"rel":"alternate","type":"text/html","href":"https:\/\/www.listendata.com\/2018\/01\/linear-regression-in-python.html","title":"A Complete Guide to Linear Regression in Python"}],"author":[{"name":{"$t":"Unknown"},"email":{"$t":"noreply@blogger.com"},"gd$image":{"rel":"http://schemas.google.com/g/2005#thumbnail","width":"16","height":"16","src":"https:\/\/img1.blogblog.com\/img\/b16-rounded.gif"}}],"media$thumbnail":{"xmlns$media":"http://search.yahoo.com/mrss/","url":"https:\/\/4.bp.blogspot.com\/-GwuWYL9RWN4\/WlXkOS72oJI\/AAAAAAAAGpU\/2TGJdC3Ni6kgsFAoyAiVaU31NghbncEUgCLcBGAs\/s72-c\/Linear%2Bregression.PNG","height":"72","width":"72"},"thr$total":{"$t":"10"}}]}});